<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>连接带有 Kerberos 认证的 Hadoop Hbase Hive Spark | Myblog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="本文操作环境为使用 Windows 远程连接集群环境，通过代码带入 Kerberos 认证信息操作集群。 项目为 Maven Project，添加依赖仅限于测试连接，如做额外开发可能会缺少依赖。请自行添加">
<meta property="og:type" content="article">
<meta property="og:title" content="连接带有 Kerberos 认证的 Hadoop Hbase Hive Spark">
<meta property="og:url" content="http://blog.iclouds.work/2017/11/21/programming/%E8%BF%9E%E6%8E%A5%E5%B8%A6%E6%9C%89%20Kerberos%20%E8%AE%A4%E8%AF%81%E7%9A%84%20Hadoop%20%20HBase%20Spark/index.html">
<meta property="og:site_name" content="Myblog">
<meta property="og:description" content="本文操作环境为使用 Windows 远程连接集群环境，通过代码带入 Kerberos 认证信息操作集群。 项目为 Maven Project，添加依赖仅限于测试连接，如做额外开发可能会缺少依赖。请自行添加">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://qiniu.iclouds.work/FgvE_RlTJ6YsxhMw7m9lbDSKLPvd.png">
<meta property="og:image" content="http://qiniu.iclouds.work/Fvr3toNl75tVQV6siXCnKq1bdQ_g.png">
<meta property="og:image" content="http://qiniu.iclouds.work/FqF9JVuTAUIj3RWpkc0lP59QtHIa.png">
<meta property="article:published_time" content="2017-11-21T10:18:08.000Z">
<meta property="article:modified_time" content="2020-05-28T02:26:00.000Z">
<meta property="article:author" content="huagang">
<meta property="article:tag" content="kerberos">
<meta property="article:tag" content="hadoop">
<meta property="article:tag" content="hbase">
<meta property="article:tag" content="hive">
<meta property="article:tag" content="spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://qiniu.iclouds.work/FgvE_RlTJ6YsxhMw7m9lbDSKLPvd.png">
  
    <link rel="alternate" href="/atom.xml" title="Myblog" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Myblog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Hello World !</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS 订阅"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://blog.iclouds.work"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-programming/连接带有 Kerberos 认证的 Hadoop  HBase Spark" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2017/11/21/programming/%E8%BF%9E%E6%8E%A5%E5%B8%A6%E6%9C%89%20Kerberos%20%E8%AE%A4%E8%AF%81%E7%9A%84%20Hadoop%20%20HBase%20Spark/" class="article-date">
  <time class="dt-published" datetime="2017-11-21T10:18:08.000Z" itemprop="datePublished">2017-11-21</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      连接带有 Kerberos 认证的 Hadoop Hbase Hive Spark
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>本文操作环境为使用 Windows 远程连接集群环境，通过代码带入 Kerberos 认证信息操作集群。</p>
<p>项目为 Maven Project，添加依赖仅限于测试连接，如做额外开发可能会缺少依赖。请自行添加</p>
<span id="more"></span>

<h2 id="操作环境"><a href="#操作环境" class="headerlink" title="操作环境"></a>操作环境</h2><ul>
<li>CentOS：7</li>
<li>Ambari：2.5.1.0</li>
<li>HDP：2.6.2.0-205</li>
<li>客户机：Windows 10</li>
</ul>
<h2 id="准备文件"><a href="#准备文件" class="headerlink" title="准备文件"></a>准备文件</h2><ul>
<li>Kerberos 配置文件。该文件一般位于 Kerberos 服务器的 <code>/etc/krb5.conf</code> 位置。将其下载到本地。</li>
<li>Hadoop 的配置文件。</li>
</ul>
<h2 id="Java-连接-Hadoop"><a href="#Java-连接-Hadoop" class="headerlink" title="Java 连接 Hadoop"></a>Java 连接 Hadoop</h2><p><a target="_blank" rel="noopener" href="https://www.2cto.com/kf/201204/126021.html">Hadoop Authentication</a></p>
<p>上面这篇博客讲解了 java 连接带有 Kerberos 的 HDFS 流程。可以看一下。</p>
<p>新建一个 Maven 项目</p>
<p><img src="http://qiniu.iclouds.work/FgvE_RlTJ6YsxhMw7m9lbDSKLPvd.png" alt="maven_project"></p>
<p>添加 HDFS 依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- HDFS dependences --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>将 <code>krb5.conf</code> <code>tendata.keytab</code>  放到项目根目录</p>
<p><img src="http://qiniu.iclouds.work/Fvr3toNl75tVQV6siXCnKq1bdQ_g.png"></p>
<p>实例代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.LocatedFileStatus;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.RemoteIterator;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.security.UserGroupInformation;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by Administrator on 2017/11/21</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HadoopAuth</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span>&#123;</span><br><span class="line">        System.setProperty(<span class="string">&quot;java.security.krb5.conf&quot;</span>, <span class="string">&quot;krb5.conf&quot;</span>);</span><br><span class="line">        conf.set(<span class="string">&quot;HadoopAuth.security.authentication&quot;</span>, <span class="string">&quot;kerberos&quot;</span>);</span><br><span class="line">        conf.set(<span class="string">&quot;fs.defaultFS&quot;</span>, <span class="string">&quot;hdfs://192.168.10.1&quot;</span>);</span><br><span class="line">        UserGroupInformation.setConfiguration(conf);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            UserGroupInformation.loginUserFromKeytab(<span class="string">&quot;tendata@TENDATA.CN&quot;</span>, <span class="string">&quot;tendata.keytab&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        FileSystem fs = FileSystem.get(conf);</span><br><span class="line">        <span class="comment">// 传入当前认证用户有操作权限的路径</span></span><br><span class="line">        Path path = <span class="keyword">new</span> Path(<span class="string">&quot;/user/tendata/testdir&quot;</span>);</span><br><span class="line">        <span class="keyword">if</span> (! fs.exists(path))&#123;</span><br><span class="line">            <span class="keyword">boolean</span> mkdirs = fs.mkdirs(path);</span><br><span class="line">            System.out.println(<span class="string">&quot;mkdir &quot;</span> + mkdirs );</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">boolean</span> delete = fs.delete(path, <span class="keyword">true</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;delete &quot;</span> + delete);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Java-连接-HBase"><a href="#Java-连接-HBase" class="headerlink" title="Java 连接 HBase"></a>Java 连接 HBase</h2><p>添加 HBase 依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- HBase dependence --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-client --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.1.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.phoenix/phoenix-core --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.7.0-HBase-1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>实例代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.ClusterStatus;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.HBaseConfiguration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.ServerName;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Admin;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Connection;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.ConnectionFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.security.UserGroupInformation;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by Administrator on 2017/11/21</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseAuth</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> String zNode = <span class="string">&quot;m1.node.hadoop,m2.node.hadoop,m3.node.hadoop&quot;</span>;</span><br><span class="line">    <span class="keyword">static</span> String zPort = <span class="string">&quot;2181&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        Configuration conf = HBaseConfiguration.create();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// krb5.conf 文件位置随意。当不在项目中的时候需要引用绝对路径</span></span><br><span class="line">        System.setProperty(<span class="string">&quot;java.security.krb5.conf&quot;</span>, <span class="string">&quot;krb5.conf&quot;</span>);     <span class="comment">// 配置系统使用 krb5.conf</span></span><br><span class="line"></span><br><span class="line">        conf.set(<span class="string">&quot;hadoop.security.authentication&quot;</span>, <span class="string">&quot;Kerberos&quot;</span>);     <span class="comment">// 指定 HDFS 安全认证方式为 Kerberos。</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 下面的对 hdfs-site.xml  的配置都可以在集群配置文件中找得到的</span></span><br><span class="line">        conf.set(<span class="string">&quot;hbase.security.authentication&quot;</span>, <span class="string">&quot;kerberos&quot;</span>);      <span class="comment">// 指定 HBase 安全认证方式为 Kerberos。</span></span><br><span class="line">        conf.set(<span class="string">&quot;hbase.master.kerberos.principal&quot;</span>, <span class="string">&quot;hbase/_HOST@TENDATA.CN&quot;</span>);</span><br><span class="line">        conf.set(<span class="string">&quot;hbase.zookeeper.property.clientPort&quot;</span>, zPort);     <span class="comment">// Zookeeper 端口</span></span><br><span class="line">        conf.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, zNode);      <span class="comment">// Zookeeper 节点</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        在查看 Zookeeper 根目录信息会有两个 hbase 开头的目录</span></span><br><span class="line"><span class="comment">        /hbase-secure   在配置安全集群下，HBase 会使用这个</span></span><br><span class="line"><span class="comment">        /hbase-unsecure 在没有配置安全集群下，使用此目录</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        conf.set(<span class="string">&quot;zookeeper.znode.parent&quot;</span>, <span class="string">&quot;/hbase-secure&quot;</span>);    <span class="comment">// 指定 HBase 在 Zookeeper 目录</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 通过hadoop security下中的 UserGroupInformation类来实现使用keytab文件登录</span></span><br><span class="line"></span><br><span class="line">        UserGroupInformation.setConfiguration(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置登录的kerberos principal和对应的 keytab 文件，其中 keytab 文件需要kdc管理员生成给到开发人员</span></span><br><span class="line">        <span class="comment">// 文件如果没法放在项目目录中，则需要引用路径</span></span><br><span class="line">        UserGroupInformation.loginUserFromKeytab(<span class="string">&quot;tendata@TENDATA.CN&quot;</span>, <span class="string">&quot;tendata.keytab&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 与HBase数据库的连接对象</span></span><br><span class="line">        Connection connection = ConnectionFactory.createConnection(conf);</span><br><span class="line">        <span class="comment">// 数据库元数据操作对象</span></span><br><span class="line">        Admin admin = connection.getAdmin();</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;---------------获取集群信息-----------------&quot;</span>);</span><br><span class="line">        ClusterStatus clusterStatus = admin.getClusterStatus();</span><br><span class="line">        String id = clusterStatus.getClusterId();</span><br><span class="line">        String hbaseVersion = clusterStatus.getHBaseVersion();</span><br><span class="line">        ServerName master = clusterStatus.getMaster();</span><br><span class="line">        System.out.println(<span class="string">&quot;id:\t&quot;</span> + id +<span class="string">&quot;\n&quot;</span> +</span><br><span class="line">                            <span class="string">&quot;version:\t&quot;</span> + hbaseVersion + <span class="string">&quot;\n&quot;</span> +</span><br><span class="line">                            <span class="string">&quot;master:\t&quot;</span> + master</span><br><span class="line">        );</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Java-连接-Hive"><a href="#Java-连接-Hive" class="headerlink" title="Java 连接 Hive"></a>Java 连接 Hive</h2><p>添加 Hive 依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Hive dependence --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-jdbc --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-common --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.httpcomponents<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>httpclient<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>实例代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.security.UserGroupInformation;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.DriverManager;</span><br><span class="line"><span class="keyword">import</span> java.sql.ResultSet;</span><br><span class="line"><span class="keyword">import</span> java.sql.Statement;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by Administrator on 2017/11/21</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HiveAuth</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String driverName = <span class="string">&quot;org.apache.hive.jdbc.HiveDriver&quot;</span>;   <span class="comment">// Hive JDBC 驱动</span></span><br><span class="line">    <span class="comment">// url ，注意后面带有 principal</span></span><br><span class="line">    <span class="comment">// 这里使用 hive2 连接</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String url = <span class="string">&quot;jdbc:hive2://m1.node.hadoop:10000/default;principal=hive/m1.node.hadoop@TENDATA.CN;&quot;</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String user = <span class="string">&quot;hive&quot;</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String password = <span class="string">&quot;&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// krb5.conf 文件位置随意。当不在项目中的时候需要引用绝对路径</span></span><br><span class="line">        System.setProperty(<span class="string">&quot;java.security.krb5.conf&quot;</span>, <span class="string">&quot;krb5.conf&quot;</span>);     <span class="comment">// 配置系统使用 krb5.conf</span></span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.set(<span class="string">&quot;hadoop.security.authentication&quot;</span>, <span class="string">&quot;kerberos&quot;</span>); <span class="comment">// 指定 hadoop 安全认证方式为 Kerberos。</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 通过hadoop security下中的 UserGroupInformation类来实现使用keytab文件登录</span></span><br><span class="line">        UserGroupInformation.setConfiguration(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置登录的 kerberos principal和对应的 keytab 文件，其中 keytab 文件需要kdc管理员生成给到开发人员</span></span><br><span class="line">        <span class="comment">// 文件如果没法放在项目目录中，则需要引用路径</span></span><br><span class="line">        UserGroupInformation.loginUserFromKeytab(<span class="string">&quot;tendata@TENDATA.CN&quot;</span>, <span class="string">&quot;tendata.keytab&quot;</span>);</span><br><span class="line"></span><br><span class="line">        Class.forName(driverName);</span><br><span class="line">        Connection conn = DriverManager.getConnection(url, user, password);</span><br><span class="line">        Statement stmt = conn.createStatement();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ResultSet res = stmt.executeQuery(<span class="string">&quot;show databases&quot;</span>);</span><br><span class="line">        <span class="keyword">while</span> (res.next()) &#123;</span><br><span class="line">            System.out.println(res.getString(<span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Java-连接-Spark"><a href="#Java-连接-Spark" class="headerlink" title="Java 连接 Spark"></a>Java 连接 Spark</h2><p>使用 Java 连接 Spark 应该是少引用了某个参数，导致指定的安全认证参数传不进去。一直认定是 SIMPLE。</p>
<p>索性直接引入 hadoop 的配置文件。</p>
<p>在 Resource 目录下放入 <code>core-site.xml</code> <code>yarn-site.xml</code> 两个文件。</p>
<p><img src="http://qiniu.iclouds.work/FqF9JVuTAUIj3RWpkc0lP59QtHIa.png"></p>
<p>添加 Spark 依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Spark dependence --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.6.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>


<p>实例代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.security.UserGroupInformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by Administrator on 2017/11/21</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkAuth</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span>&#123;</span><br><span class="line">        <span class="comment">// 这里要给系统加载 krb5 的配置，否则系统会使用默认 remal</span></span><br><span class="line">        System.setProperty(<span class="string">&quot;java.security.krb5.conf&quot;</span>, <span class="string">&quot;krb5.conf&quot;</span>);</span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">// 下面的对 hdfs-site.xml  的配置都可以在集群配置文件中找得到的</span></span><br><span class="line">        configuration.set(<span class="string">&quot;hadoop.security.authentication&quot;</span>, <span class="string">&quot;kerberos&quot;</span>);     <span class="comment">// 指定 hadoop 安全认证方式为 Kerberos。</span></span><br><span class="line">        configuration.set(<span class="string">&quot;dfs.namenode.kerberos.principal.pattern&quot;</span>, <span class="string">&quot;nn/*@TENDATA.CN&quot;</span>);</span><br><span class="line">        configuration.set(<span class="string">&quot;fs.defaultFS&quot;</span>, <span class="string">&quot;hdfs://192.168.10.1&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 通过hadoop security下中的 UserGroupInformation类来实现使用keytab文件登录</span></span><br><span class="line">        UserGroupInformation.setConfiguration(configuration);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 设置登录的kerberos principal和对应的 keytab 文件，其中 keytab 文件需要kdc管理员生成给到开发人员</span></span><br><span class="line">            <span class="comment">// 文件如果没法放在项目目录中，则需要引用路径</span></span><br><span class="line">            UserGroupInformation.loginUserFromKeytab(<span class="string">&quot;tendata@TENDATA.CN&quot;</span>, <span class="string">&quot;tendata.keytab&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf().setAppName(<span class="string">&quot;Test_Java_E_Kevin&quot;</span>).setMaster(<span class="string">&quot;local&quot;</span>);</span><br><span class="line"></span><br><span class="line">        JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">        JavaRDD&lt;String&gt; stringJavaRDD = sc.textFile(<span class="string">&quot;hdfs://m1.node.hadoop:8020/user/tendata/tmp/tmp-kevin/example/data/example_data_id_name.txt&quot;</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        List&lt;String&gt; collect = stringJavaRDD.collect();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (String i : collect) &#123;</span><br><span class="line">            System.out.println(i);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Scala-连接-Spark"><a href="#Scala-连接-Spark" class="headerlink" title="Scala 连接 Spark"></a>Scala 连接 Spark</h2><p>针对于 Scala Spark 的环境请自行配置，具体连接代码如下。</p>
<p>需要在 Resource 目录下引用前面提到的两个配置文件</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.security.<span class="type">UserGroupInformation</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SQLContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkContext</span>, <span class="type">SparkConf</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Put</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="type">System</span>.setProperty(<span class="string">&quot;java.security.krb5.conf&quot;</span>, <span class="string">&quot;krb5.conf&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">Configuration</span>()</span><br><span class="line">    conf.set(<span class="string">&quot;hadoop.security.authentication&quot;</span>, <span class="string">&quot;Kerberos&quot;</span>)</span><br><span class="line">    conf.set(<span class="string">&quot;fs.defaultFS&quot;</span>, <span class="string">&quot;hdfs://192.168.10.1&quot;</span>)</span><br><span class="line">    conf.set(<span class="string">&quot;dfs.namenode.kerberos.principal.pattern&quot;</span>, <span class="string">&quot;nn/*@TENDATA.CN&quot;</span>)</span><br><span class="line">    <span class="type">UserGroupInformation</span>.setConfiguration(conf)</span><br><span class="line">    <span class="type">UserGroupInformation</span>.loginUserFromKeytab(<span class="string">&quot;tendata@TENDATA.CN&quot;</span>, <span class="string">&quot;tendata.keytab&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;Test_SparkReadHDFS_Kevin&quot;</span>).setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line">    <span class="keyword">val</span> scRDD = sc.textFile(<span class="string">&quot;hdfs://m1.node.hadoop:8020/user/tendata/tmp/tmp-kevin/example/data/example_data_id_name.txt&quot;</span>, <span class="number">1</span>);</span><br><span class="line">    scRDD.collect().foreach(x =&gt; println(x))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="错误总结"><a href="#错误总结" class="headerlink" title="错误总结"></a>错误总结</h2><h5 id="1-Can’t-get-Kerberos-realm"><a href="#1-Can’t-get-Kerberos-realm" class="headerlink" title="1.  Can’t get Kerberos realm"></a>1.  Can’t get Kerberos realm</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: Can&#39;t get Kerberos realm</span><br><span class="line">    at org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:65)</span><br><span class="line">    at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:249)</span><br><span class="line">    at org.apache.hadoop.security.UserGroupInformation.setConfiguration(UserGroupInformation.java:285)</span><br><span class="line">    at HdfsConnKerberos.HDFSClient.main(HDFSClient.java:43)</span><br><span class="line">    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">    at java.lang.reflect.Method.invoke(Method.java:497)</span><br><span class="line">    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)</span><br><span class="line">Caused by: java.lang.reflect.InvocationTargetException</span><br><span class="line">    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">    at java.lang.reflect.Method.invoke(Method.java:497)</span><br><span class="line">    at org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:84)</span><br><span class="line">    at org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:63)</span><br><span class="line">    ... 8 more</span><br><span class="line">Caused by: KrbException: Cannot locate default realm</span><br><span class="line">    at sun.security.krb5.Config.getDefaultRealm(Config.java:1006)</span><br><span class="line">    ... 14 more</span><br><span class="line">Caused by: KrbException: Generic error (description in e-text) (60) - Unable to locate Kerberos realm</span><br><span class="line">    at sun.security.krb5.Config.getRealmFromDNS(Config.java:1102)</span><br><span class="line">    at sun.security.krb5.Config.getDefaultRealm(Config.java:987)</span><br><span class="line">    ... 14 more</span><br></pre></td></tr></table></figure>

<p>主要几点</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Caused by: KrbException: Cannot locate default realm</span><br><span class="line"></span><br><span class="line">Caused by: KrbException: Generic error (description in e-text) (60) - Unable to locate Kerberos realm</span><br></pre></td></tr></table></figure>

<p>主要原因在代码中没有添加 <code>krb5.conf</code> 这个配置，所以检查这个配置文件的是否存在和文件内容的正确性</p>
<h5 id="2-org-apache-hadoop-hbase-exceptions-ConnectionClosingException-Call-to-m1-node-hadoop-192-168-10-1-16000-failed"><a href="#2-org-apache-hadoop-hbase-exceptions-ConnectionClosingException-Call-to-m1-node-hadoop-192-168-10-1-16000-failed" class="headerlink" title="2. org.apache.hadoop.hbase.exceptions.ConnectionClosingException: Call to m1.node.hadoop/192.168.10.1:16000 failed"></a>2. org.apache.hadoop.hbase.exceptions.ConnectionClosingException: Call to m1.node.hadoop/192.168.10.1:16000 failed</h5><p>读取 HBase 信息，连接到 HBase 没反应，重复出现下面的信息。</p>
<p>Call exception, tries=10, retries=31, started=48283 ms ago, cancelled=false, msg=com.google.protobuf.ServiceException: org.apache.hadoop.hbase.exceptions.ConnectionClosingException: Call to m1.node.hadoop/192.168.10.1:16000 failed on local exception: org.apache.hadoop.hbase.exceptions.ConnectionClosingException: Connection to m1.node.hadoop/192.168.10.1:16000 is closing. Call id=10, waitTime=1</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">2017-10-13 15:26:02,021 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:&lt;init&gt;(438)) - Initiating client connection, connectString&#x3D;m1.node.hadoop:2181,m2.node.hadoop:2181,m3.node.hadoop:2181 sessionTimeout&#x3D;180000 watcher&#x3D;org.apache.hadoop.hbase.zookeeper.PendingWatcher@65b3f4a4</span><br><span class="line">2017-10-13 15:26:02,077 INFO  [main-SendThread(m3.node.hadoop:2181)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1019)) - Opening socket connection to server m3.node.hadoop&#x2F;192.168.10.3:2181. Will not attempt to authenticate using SASL (unknown error)</span><br><span class="line">2017-10-13 15:26:02,079 INFO  [main-SendThread(m3.node.hadoop:2181)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(864)) - Socket connection established, initiating session, client: &#x2F;192.168.2.199:59415, server: m3.node.hadoop&#x2F;192.168.10.3:2181</span><br><span class="line">2017-10-13 15:26:02,104 INFO  [main-SendThread(m3.node.hadoop:2181)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1279)) - Session establishment complete on server m3.node.hadoop&#x2F;192.168.10.3:2181, sessionid &#x3D; 0x35efece455700dd, negotiated timeout &#x3D; 40000</span><br><span class="line">2017-10-13 15:26:05,349 WARN  [main]  (NetworkAddressUtils.java:getLocalIpAddress(389)) - Your hostname, DESKTOP-5KM5T43 resolves to a loopback&#x2F;non-reachable address: fe80:0:0:0:3433:e0f1:9aa7:18da%net4, but we couldn&#39;t find any external IP address!</span><br><span class="line">2017-10-13 15:26:06,524 WARN  [main] shortcircuit.DomainSocketFactory (DomainSocketFactory.java:&lt;init&gt;(117)) - The short-circuit local reads feature cannot be used because UNIX Domain sockets are not available on Windows.</span><br><span class="line">2017-10-13 15:26:54,855 INFO  [main] client.RpcRetryingCaller (RpcRetryingCaller.java:callWithRetries(146)) - Call exception, tries&#x3D;10, retries&#x3D;31, started&#x3D;48283 ms ago, cancelled&#x3D;false, msg&#x3D;com.google.protobuf.ServiceException: org.apache.hadoop.hbase.exceptions.ConnectionClosingException: Call to m1.node.hadoop&#x2F;192.168.10.1:16000 failed on local exception: org.apache.hadoop.hbase.exceptions.ConnectionClosingException: Connection to m1.node.hadoop&#x2F;192.168.10.1:16000 is closing. Call id&#x3D;10, waitTime&#x3D;1 </span><br><span class="line">2017-10-13 15:27:15,012 INFO  [main] client.RpcRetryingCaller (RpcRetryingCaller.java:callWithRetries(146)) - Call exception, tries&#x3D;11, retries&#x3D;31, started&#x3D;68440 ms ago, cancelled&#x3D;false, msg&#x3D;com.google.protobuf.ServiceException: org.apache.hadoop.hbase.exceptions.ConnectionClosingException: Call to m1.node.hadoop&#x2F;192.168.10.1:16000 failed on local exception: org.apache.hadoop.hbase.exceptions.ConnectionClosingException: Connection to m1.node.hadoop&#x2F;192.168.10.1:16000 is closing. Call id&#x3D;11, waitTime&#x3D;1 </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这种情况是在创建 Configuration 的格式不对造成。</p>
<p>下面两种获取 Configuration 对象是不同的。</p>
<p>如果使用第一种 conf 操作 HBase，则会出现上述错误。</p>
<p>而正确的创建方式应该是第二种。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">Configuration conf = HBaseConfiguration.create();</span><br></pre></td></tr></table></figure>

<h5 id="3-org-apache-hadoop-hbase-client-RpcRetryingCaller-Call-exception-tries-11-retries-35-started-48413-ms-ago-cancelled-false-msg"><a href="#3-org-apache-hadoop-hbase-client-RpcRetryingCaller-Call-exception-tries-11-retries-35-started-48413-ms-ago-cancelled-false-msg" class="headerlink" title="3.  [org.apache.hadoop.hbase.client.RpcRetryingCaller] - Call exception, tries=11, retries=35, started=48413 ms ago, cancelled=false, msg="></a>3.  [org.apache.hadoop.hbase.client.RpcRetryingCaller] - Call exception, tries=11, retries=35, started=48413 ms ago, cancelled=false, msg=</h5><p>连接 HBase 没有反应，重复出现 Call exception</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">2017-11-21 17:45:11,982 INFO [org.apache.zookeeper.ZooKeeper] - Initiating client connection, connectString&#x3D;m1.node.hadoop:2181,m2.node.hadoop:2181,m3.node.hadoop:2181 sessionTimeout&#x3D;90000 watcher&#x3D;hconnection-0x66d189790x0, quorum&#x3D;m1.node.hadoop:2181,m2.node.hadoop:2181,m3.node.hadoop:2181, baseZNode&#x3D;&#x2F;hbase-secure</span><br><span class="line">2017-11-21 17:45:12,049 INFO [org.apache.zookeeper.ClientCnxn] - Opening socket connection to server m2.node.hadoop&#x2F;192.168.10.2:2181. Will not attempt to authenticate using SASL (unknown error)</span><br><span class="line">2017-11-21 17:45:12,050 INFO [org.apache.zookeeper.ClientCnxn] - Socket connection established to m2.node.hadoop&#x2F;192.168.10.2:2181, initiating session</span><br><span class="line">2017-11-21 17:45:12,065 INFO [org.apache.zookeeper.ClientCnxn] - Session establishment complete on server m2.node.hadoop&#x2F;192.168.10.2:2181, sessionid &#x3D; 0x25fdc95c59d001d, negotiated timeout &#x3D; 40000</span><br><span class="line">2017-11-21 17:45:15,386 WARN [] - Your hostname, DESKTOP-5KM5T43 resolves to a loopback&#x2F;non-reachable address: fe80:0:0:0:3c56:c61d:8b18:745%net4, but we couldn&#39;t find any external IP address!</span><br><span class="line">---------------获取集群信息-----------------</span><br><span class="line">2017-11-21 17:45:54,986 INFO [org.apache.hadoop.hbase.client.RpcRetryingCaller] - Call exception, tries&#x3D;10, retries&#x3D;35, started&#x3D;38401 ms ago, cancelled&#x3D;false, msg&#x3D;</span><br><span class="line">2017-11-21 17:46:04,998 INFO [org.apache.hadoop.hbase.client.RpcRetryingCaller] - Call exception, tries&#x3D;11, retries&#x3D;35, started&#x3D;48413 ms ago, cancelled&#x3D;false, msg&#x3D;</span><br></pre></td></tr></table></figure>

<p>这是没有指定 HBase 安全认证导致的。</p>
<p>增加如下配置即可</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conf.set(<span class="string">&quot;hbase.security.authentication&quot;</span>, <span class="string">&quot;kerberos&quot;</span>);      <span class="comment">// 指定 HBase 安全认证方式为 Kerberos。</span></span><br></pre></td></tr></table></figure>

<h5 id="4-java-io-IOException-java-lang-reflect-InvocationTargetException"><a href="#4-java-io-IOException-java-lang-reflect-InvocationTargetException" class="headerlink" title="4. java.io.IOException: java.lang.reflect.InvocationTargetException"></a>4. java.io.IOException: java.lang.reflect.InvocationTargetException</h5><p>连接 HBase 出现 <code>org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory</code> 类找不到</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread <span class="string">&quot;main&quot;</span> java.io.IOException: java.lang.reflect.InvocationTargetException</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:<span class="number">240</span>)</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:<span class="number">218</span>)</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:<span class="number">119</span>)</span><br><span class="line">    at HBaseAuth.main(HBaseAuth.java:<span class="number">33</span>)</span><br><span class="line">    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:<span class="number">62</span>)</span><br><span class="line">    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:<span class="number">43</span>)</span><br><span class="line">    at java.lang.reflect.Method.invoke(Method.java:<span class="number">497</span>)</span><br><span class="line">    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:<span class="number">144</span>)</span><br><span class="line">Caused by: java.lang.reflect.InvocationTargetException</span><br><span class="line">    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class="line">    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:<span class="number">62</span>)</span><br><span class="line">    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:<span class="number">45</span>)</span><br><span class="line">    at java.lang.reflect.Constructor.newInstance(Constructor.java:<span class="number">422</span>)</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:<span class="number">238</span>)</span><br><span class="line">    ... <span class="number">8</span> more</span><br><span class="line">Caused by: java.lang.UnsupportedOperationException: Unable to find org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory</span><br><span class="line">    at org.apache.hadoop.hbase.util.ReflectionUtils.instantiateWithCustomCtor(ReflectionUtils.java:<span class="number">36</span>)</span><br><span class="line">    at org.apache.hadoop.hbase.ipc.RpcControllerFactory.instantiate(RpcControllerFactory.java:<span class="number">58</span>)</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.createAsyncProcess(ConnectionManager.java:<span class="number">2256</span>)</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.&lt;init&gt;(ConnectionManager.java:<span class="number">691</span>)</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.&lt;init&gt;(ConnectionManager.java:<span class="number">631</span>)</span><br><span class="line">    ... <span class="number">13</span> more</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory</span><br><span class="line">    at java.net.URLClassLoader.findClass(URLClassLoader.java:<span class="number">381</span>)</span><br><span class="line">    at java.lang.ClassLoader.loadClass(ClassLoader.java:<span class="number">424</span>)</span><br><span class="line">    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:<span class="number">331</span>)</span><br><span class="line">    at java.lang.ClassLoader.loadClass(ClassLoader.java:<span class="number">357</span>)</span><br><span class="line">    at java.lang.Class.forName0(Native Method)</span><br><span class="line">    at java.lang.Class.forName(Class.java:<span class="number">264</span>)</span><br><span class="line">    at org.apache.hadoop.hbase.util.ReflectionUtils.instantiateWithCustomCtor(ReflectionUtils.java:<span class="number">32</span>)</span><br><span class="line">    ... <span class="number">17</span> more</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/itboys/p/6862366.html">https://www.cnblogs.com/itboys/p/6862366.html</a></p>
<ol>
<li><p>检查应用开发工程的配置文件hbase-site.xml中是否包含配置项hbase.rpc.controllerfactory.class。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;name&gt;hbase.rpc.controllerfactory.class&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory&lt;&#x2F;value&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>如果当前的应用开发工程配置项中包含该配置项，则应用开发程序还需要引入Jar包“phoenix-core-4.4.0-HBase-1.0.jar”。此Jar包可以从HBase客户端安装目录下的“HBase/hbase/lib”获取。</p>
</li>
<li><p>如果不想引入该Jar包，请将应用开发工程的配置文件“hbase-site.xml”中的配置“hbase.rpc.controllerfactory.class”删除掉。</p>
</li>
</ol>
<h5 id="5-SIMPLE-authentication-is-not-enabled-Available-TOKEN-KERBEROS"><a href="#5-SIMPLE-authentication-is-not-enabled-Available-TOKEN-KERBEROS" class="headerlink" title="5. SIMPLE authentication is not enabled.  Available:[TOKEN, KERBEROS]"></a>5. SIMPLE authentication is not enabled.  Available:[TOKEN, KERBEROS]</h5><p>这个问题我只在使用 Spark 的情况下出现过。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled.  Available:[TOKEN, KERBEROS]</span><br><span class="line">    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class="line">    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)</span><br><span class="line">    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)</span><br><span class="line">    at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)</span><br><span class="line">    at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)</span><br><span class="line">    at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2110)</span><br><span class="line">    at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1305)</span><br><span class="line">    at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)</span><br><span class="line">    at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)</span><br><span class="line">    at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1317)</span><br><span class="line">    at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:57)</span><br><span class="line">    at org.apache.hadoop.fs.Globber.glob(Globber.java:252)</span><br><span class="line">    at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1674)</span><br><span class="line">    at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:259)</span><br><span class="line">    at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)</span><br><span class="line">    at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)</span><br><span class="line">    at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)</span><br><span class="line">    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)</span><br><span class="line">    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)</span><br><span class="line">    at scala.Option.getOrElse(Option.scala:121)</span><br><span class="line">    at org.apache.spark.rdd.RDD.partitions(RDD.scala:246)</span><br><span class="line">    at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)</span><br><span class="line">    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)</span><br><span class="line">    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)</span><br><span class="line">    at scala.Option.getOrElse(Option.scala:121)</span><br><span class="line">    at org.apache.spark.rdd.RDD.partitions(RDD.scala:246)</span><br><span class="line">    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1911)</span><br><span class="line">    at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:875)</span><br><span class="line">    at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:873)</span><br><span class="line">    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)</span><br><span class="line">    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)</span><br><span class="line">    at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)</span><br><span class="line">    at org.apache.spark.rdd.RDD.foreach(RDD.scala:873)</span><br><span class="line">    at HDFS.SparkHDFS$.main(SparkHDFS.scala:43)</span><br><span class="line">    at HDFS.SparkHDFS.main(SparkHDFS.scala)</span><br><span class="line">    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">    at java.lang.reflect.Method.invoke(Method.java:497)</span><br><span class="line">    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)</span><br><span class="line">Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): SIMPLE authentication is not enabled.  Available:[TOKEN, KERBEROS]</span><br><span class="line">    at org.apache.hadoop.ipc.Client.call(Client.java:1475)</span><br><span class="line">    at org.apache.hadoop.ipc.Client.call(Client.java:1412)</span><br><span class="line">    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)</span><br><span class="line">    at com.sun.proxy.$Proxy20.getFileInfo(Unknown Source)</span><br><span class="line">    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)</span><br><span class="line">    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">    at java.lang.reflect.Method.invoke(Method.java:497)</span><br><span class="line">    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)</span><br><span class="line">    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)</span><br><span class="line">    at com.sun.proxy.$Proxy21.getFileInfo(Unknown Source)</span><br><span class="line">    at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2108)</span><br><span class="line">    ... 34 more</span><br></pre></td></tr></table></figure>

<p>直接在 Resource 目录下加入 <code>core-site.xml</code> 配置文件即可。</p>
<p>当人加入了上述配置文件后，重新运行又会出现下面的错误</p>
<h5 id="6-Can’t-get-Master-Kerberos-principal-for-use-as-renewer"><a href="#6-Can’t-get-Master-Kerberos-principal-for-use-as-renewer" class="headerlink" title="6.  Can’t get Master Kerberos principal for use as renewer"></a>6.  Can’t get Master Kerberos principal for use as renewer</h5><p>记不得这个错误有没有在其他地方出现过了。反正现在在使用 Spark 的时候出现了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.io.IOException: Can&#39;t get Master Kerberos principal for use as renewer</span><br><span class="line">    at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:116)</span><br><span class="line">    at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:100)</span><br><span class="line">    at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80)</span><br><span class="line">    at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:205)</span><br><span class="line">    at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)</span><br><span class="line">    at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:202)</span><br><span class="line">    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)</span><br><span class="line">    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)</span><br><span class="line">    at scala.Option.getOrElse(Option.scala:121)</span><br><span class="line">    at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)</span><br><span class="line">    at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)</span><br><span class="line">    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)</span><br><span class="line">    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)</span><br><span class="line">    at scala.Option.getOrElse(Option.scala:121)</span><br><span class="line">    at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)</span><br><span class="line">    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)</span><br><span class="line">    at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)</span><br><span class="line">    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)</span><br><span class="line">    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)</span><br><span class="line">    at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)</span><br><span class="line">    at org.apache.spark.rdd.RDD.collect(RDD.scala:926)</span><br><span class="line">    at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:339)</span><br><span class="line">    at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:46)</span><br><span class="line">    at SparkAuth.main(SparkAuth.java:43)</span><br><span class="line">    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">    at java.lang.reflect.Method.invoke(Method.java:497)</span><br><span class="line">    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)</span><br></pre></td></tr></table></figure>

<p>解决办法：</p>
<p>在 Resource 目录下引入 <code>yarn-site.xml</code> 配置文件</p>
<p><strong>针对前两个错误，主要在 Spark 中出现。本人猜测，可能是因为在 Windows 端运行 local 模式程序的时候，本地作为 Driver，当 Executor 端 真正去访问 HDFS 中的资源的时候， Executor 并没有拿到认证身份。所以，在加入配置文件后， Executor 端会通过配置去相应位置使用 keytab 获取 kgt ，然后正常访问集群中的资源。</strong></p>
<blockquote>
<p>Update 2020-04-17 16:22:00</p>
</blockquote>
<p>在开发端远程提交 mapreduce 任务的时候同样出现了这个问题。在此确认是由于 yarn 的认证问题。</p>
<p>解决方案：</p>
<p>增加 <code>yarn</code> 相关 principal。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">configuration.set(<span class="string">&quot;yarn.nodemanager.principal&quot;</span>, <span class="string">&quot;nm/_HOST@TENDATA.CN&quot;</span>);</span><br><span class="line">configuration.set(<span class="string">&quot;yarn.resourcemanager.principal&quot;</span>, <span class="string">&quot;rm/_HOST@TENDATA.CN&quot;</span>);</span><br></pre></td></tr></table></figure>

<h5 id="Secure-IO-is-not-possible-without-native-code-extensions"><a href="#Secure-IO-is-not-possible-without-native-code-extensions" class="headerlink" title="Secure IO is not possible without native code extensions."></a>Secure IO is not possible without native code extensions.</h5><p>运行 mapreduce 任务，出现如下问题</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">java.lang.Exception: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in localfetcher#1</span><br><span class="line">    at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:492)</span><br><span class="line">    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:559)</span><br><span class="line">Caused by: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in localfetcher#1</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)</span><br><span class="line">    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:377)</span><br><span class="line">    at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:347)</span><br><span class="line">    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</span><br><span class="line">    at java.util.concurrent.FutureTask.run(FutureTask.java:266)</span><br><span class="line">    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">    at java.lang.Thread.run(Thread.java:748)</span><br><span class="line">Caused by: java.lang.ExceptionInInitializerError</span><br><span class="line">    at org.apache.hadoop.mapred.SpillRecord.&lt;init&gt;(SpillRecord.java:71)</span><br><span class="line">    at org.apache.hadoop.mapred.SpillRecord.&lt;init&gt;(SpillRecord.java:62)</span><br><span class="line">    at org.apache.hadoop.mapred.SpillRecord.&lt;init&gt;(SpillRecord.java:57)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.LocalFetcher.copyMapOutput(LocalFetcher.java:125)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.LocalFetcher.doCopy(LocalFetcher.java:103)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.LocalFetcher.run(LocalFetcher.java:86)</span><br><span class="line">Caused by: java.lang.RuntimeException: Secure IO is not possible without native code extensions.</span><br><span class="line">    at org.apache.hadoop.io.SecureIOUtils.&lt;clinit&gt;(SecureIOUtils.java:71)</span><br><span class="line">    ... 6 more</span><br><span class="line">2020-04-17 20:16</span><br></pre></td></tr></table></figure>

<p>这个问题是由于没有使用 <code>Tative library</code> 导致的，在程序运行第一行一般会有个警告 <code>Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</code><br>这是没有加载动态库。一般情况下 Hadoop 的二进制报已经包含了，只是没有在系统中加载。</p>
<p>动态库在 <code>$HADOOP_HOME/lib/native</code> 里面，没有的要去找一下，或者自己编译。</p>
<p>有了动态库还需要在系统中配置，推荐是配置环境变量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_COMMON_LIB_NATIVE_DIR&#x3D;$HADOOP_HOME&#x2F;lib&#x2F;native</span><br><span class="line">export LD_LIBRARY_PATH&#x3D;$LD_LIBRARY_PATH:$HADOOP_COMMON_LIB_NATIVE_DIR</span><br></pre></td></tr></table></figure>

<p>环境变量生效后就可以看到警告消除，而且程序也不会出现上面提到的问题啦。</p>
<h5 id="Exceeded-MAX-FAILED-UNIQUE-FETCHES-bailing-out"><a href="#Exceeded-MAX-FAILED-UNIQUE-FETCHES-bailing-out" class="headerlink" title="Exceeded MAX_FAILED_UNIQUE_FETCHES; bailing-out"></a>Exceeded MAX_FAILED_UNIQUE_FETCHES; bailing-out</h5><p>这个问题是在使用了 Kerberos 认证的时候出现的，如果没有使用认证也出现了请参考 <a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/24066128/hadoop-error-in-shuffle-in-fetcher-exceeded-max-failed-unique-fetches">Hadoop error in shuffle in fetcher: Exceeded MAX_FAILED_UNIQUE_FETCHES</a> 解决。</p>
<p>出现这个问题是在集群客户端使用 <code>yarn jar demo.jar</code> 在集群中运行 mapreduce 任务的时候报错了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#30</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)</span><br><span class="line">    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:377)</span><br><span class="line">    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)</span><br><span class="line">    at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">    at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)</span><br><span class="line">    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)</span><br><span class="line">Caused by: java.io.IOException: Exceeded MAX_FAILED_UNIQUE_FETCHES; bailing-out.</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.checkReducerHealth(ShuffleSchedulerImpl.java:396)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.copyFailed(ShuffleSchedulerImpl.java:311)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:361)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:198)</span><br></pre></td></tr></table></figure>

<p>当时想到运行 mapreduce 任务需要增加相关认证信息，但是没有加完整。完整认证代码如下</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> demo.utils;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.security.UserGroupInformation;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Auth</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String keytab;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Auth</span><span class="params">(String keytab)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.keytab = keytab;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">authorization</span><span class="params">(Configuration configuration)</span> </span>&#123;</span><br><span class="line">        System.setProperty(<span class="string">&quot;java.security.krb5.conf&quot;</span>, <span class="string">&quot;/etc/krb5.conf&quot;</span>);</span><br><span class="line">        configuration.set(<span class="string">&quot;hadoop.security.authentication&quot;</span>, <span class="string">&quot;Kerberos&quot;</span>);</span><br><span class="line">        configuration.set(<span class="string">&quot;fs.defaultFS&quot;</span>, <span class="string">&quot;hdfs://m1.node.hadoop&quot;</span>);</span><br><span class="line">        configuration.set(<span class="string">&quot;dfs.namenode.kerberos.principal.pattern&quot;</span>, <span class="string">&quot;nn/*@TENDATA.CN&quot;</span>);</span><br><span class="line">        configuration.set(<span class="string">&quot;yarn.nodemanager.principal&quot;</span>, <span class="string">&quot;nm/_HOST@TENDATA.CN&quot;</span>);</span><br><span class="line">        configuration.set(<span class="string">&quot;yarn.resourcemanager.principal&quot;</span>, <span class="string">&quot;rm/_HOST@TENDATA.CN&quot;</span>);</span><br><span class="line">        configuration.set(<span class="string">&quot;yarn.nodemanager.webapp.spnego-principal&quot;</span>, <span class="string">&quot;HTTP/_HOST@TENDATA.CN&quot;</span>);     <span class="comment">// 集群运行的时候一定要加，在执行 shuffle 的时候会在集群间交换数据</span></span><br><span class="line">        configuration.set(<span class="string">&quot;yarn.resourcemanager.webapp.spnego-principal&quot;</span>, <span class="string">&quot;HTTP/_HOST@TENDATA.CN&quot;</span>); <span class="comment">// 这也也要加上</span></span><br><span class="line">        UserGroupInformation.setConfiguration(configuration);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            UserGroupInformation.setConfiguration(configuration);</span><br><span class="line">            UserGroupInformation.loginUserFromKeytab(<span class="string">&quot;whg@TENDATA.CN&quot;</span>, <span class="keyword">this</span>.keytab);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>代码如上，在运行 mapreduce 任务的时候除了增加 <code>yarn.nodemanager.principal</code> 和 <code>yarn.resourcemanager.principal</code> 的配置还要增加 <code>yarn.nodemanager.webapp.spnego-principal</code><br> 和 <code>yarn.resourcemanager.webapp.spnego-principal</code> 的配置，否则集群间交换数据会报上面的问题。在开发环境即 local 模式运行的时候可以不用后面两个。</p>
<h5 id="7-终极解决问题"><a href="#7-终极解决问题" class="headerlink" title="7. 终极解决问题"></a>7. 终极解决问题</h5><p>在 Resource 目录引入所使用的服务的所有配置。比如 hadoop 的四个配置文件。</p>
<p>然后使用代码直接加载配置</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">configuration.addResource(<span class="string">&quot;core-site.xml&quot;</span>);</span><br><span class="line">configuration.addResource(<span class="string">&quot;hdfs-site.xml&quot;</span>);</span><br><span class="line">configuration.addResource(<span class="string">&quot;mapred-site.xml&quot;</span>);</span><br><span class="line">configuration.addResource(<span class="string">&quot;yarn-site.xml&quot;</span>);</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://blog.iclouds.work/2017/11/21/programming/%E8%BF%9E%E6%8E%A5%E5%B8%A6%E6%9C%89%20Kerberos%20%E8%AE%A4%E8%AF%81%E7%9A%84%20Hadoop%20%20HBase%20Spark/" data-id="ckmwwmzsm0021eqnkbsro7hmy" data-title="连接带有 Kerberos 认证的 Hadoop Hbase Hive Spark" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hadoop/" rel="tag">hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hbase/" rel="tag">hbase</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hive/" rel="tag">hive</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kerberos/" rel="tag">kerberos</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/03/23/operations/Deepin%20%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">前一篇</strong>
      <div class="article-nav-title">
        
          Deepin 使用记录
        
      </div>
    </a>
  
  
    <a href="/2017/11/20/operations/Ambari%20%E6%95%B4%E5%90%88%20Kerberos%20+%20Openldap/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">后一篇</strong>
      <div class="article-nav-title">Ambari 整合 Kerberos + Openldap</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/CloudNative/">CloudNative</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Documents/">Documents</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hadoop/">Hadoop</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BC%80%E5%8F%91/">开发</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%BB%B4/">运维</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Jupyter-notebook/" rel="tag">Jupyter notebook</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kerberos/" rel="tag">Kerberos</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ambari/" rel="tag">ambari</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/api-desig/" rel="tag">api-desig</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/api-design/" rel="tag">api-design</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/centos/" rel="tag">centos</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/deepin/" rel="tag">deepin</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/fastapi/" rel="tag">fastapi</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/gitlab-ci/" rel="tag">gitlab-ci</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hadoop/" rel="tag">hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hbase/" rel="tag">hbase</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hive/" rel="tag">hive</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kerberos/" rel="tag">kerberos</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/lvm/" rel="tag">lvm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/openldap/" rel="tag">openldap</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/react/" rel="tag">react</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/" rel="tag">spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/zabbix/" rel="tag">zabbix</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/Jupyter-notebook/" style="font-size: 10px;">Jupyter notebook</a> <a href="/tags/Kerberos/" style="font-size: 10px;">Kerberos</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/ambari/" style="font-size: 10px;">ambari</a> <a href="/tags/api-desig/" style="font-size: 10px;">api-desig</a> <a href="/tags/api-design/" style="font-size: 20px;">api-design</a> <a href="/tags/centos/" style="font-size: 17.5px;">centos</a> <a href="/tags/deepin/" style="font-size: 10px;">deepin</a> <a href="/tags/fastapi/" style="font-size: 10px;">fastapi</a> <a href="/tags/gitlab-ci/" style="font-size: 10px;">gitlab-ci</a> <a href="/tags/hadoop/" style="font-size: 10px;">hadoop</a> <a href="/tags/hbase/" style="font-size: 10px;">hbase</a> <a href="/tags/hive/" style="font-size: 10px;">hive</a> <a href="/tags/kerberos/" style="font-size: 15px;">kerberos</a> <a href="/tags/lvm/" style="font-size: 10px;">lvm</a> <a href="/tags/openldap/" style="font-size: 12.5px;">openldap</a> <a href="/tags/python/" style="font-size: 12.5px;">python</a> <a href="/tags/react/" style="font-size: 10px;">react</a> <a href="/tags/spark/" style="font-size: 10px;">spark</a> <a href="/tags/zabbix/" style="font-size: 10px;">zabbix</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">九月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">八月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">五月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">五月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">十月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">八月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">四月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">三月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">十一月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">二月 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/09/01/api-design/microsoft_api_guide/">Microsoft REST API 准则</a>
          </li>
        
          <li>
            <a href="/2020/09/01/api-design/paypal_api_style_guide/">PayPal API 设计原则</a>
          </li>
        
          <li>
            <a href="/2020/08/28/api-design/paypal_api_first_part3/">大规模分解API优先转换。在PayPal学习的经验教训–第3部分</a>
          </li>
        
          <li>
            <a href="/2020/08/28/api-design/paypal_api_first_part2/">大规模分解API优先转换。在PayPal学习的经验教训–第2部分</a>
          </li>
        
          <li>
            <a href="/2020/08/27/api-design/paypal_api_first_part1/">大规模分解API优先转换。在PayPal学习的经验教训–第1部分</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 huagang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>