<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="G9EWiZRZAkmLhkNRG7gEmOIhxaUUbPULBVwc7R8ik2Y">
  <meta name="msvalidate.01" content="826E9F7DFD792263CCD9EEA67320505D">
  <meta name="baidu-site-verification" content="code-qyVo5emTxH">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"blog.iclouds.work","root":"/","images":"/images","scheme":"Mist","darkmode":true,"version":"8.8.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>
<meta name="description" content="本文操作环境为使用 Windows 远程连接集群环境，通过代码带入 Kerberos 认证信息操作集群。 项目为 Maven Project，添加依赖仅限于测试连接，如做额外开发可能会缺少依赖。请自行添加">
<meta property="og:type" content="article">
<meta property="og:title" content="连接带有 Kerberos 认证的 Hadoop Hbase Hive Spark">
<meta property="og:url" content="http://blog.iclouds.work/2017/11/21/programming/%E8%BF%9E%E6%8E%A5%E5%B8%A6%E6%9C%89%20Kerberos%20%E8%AE%A4%E8%AF%81%E7%9A%84%20Hadoop%20%20HBase%20Spark/index.html">
<meta property="og:site_name" content="Myblog">
<meta property="og:description" content="本文操作环境为使用 Windows 远程连接集群环境，通过代码带入 Kerberos 认证信息操作集群。 项目为 Maven Project，添加依赖仅限于测试连接，如做额外开发可能会缺少依赖。请自行添加">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://qiniu.iclouds.work/FgvE_RlTJ6YsxhMw7m9lbDSKLPvd.png">
<meta property="og:image" content="http://qiniu.iclouds.work/Fvr3toNl75tVQV6siXCnKq1bdQ_g.png">
<meta property="og:image" content="http://qiniu.iclouds.work/FqF9JVuTAUIj3RWpkc0lP59QtHIa.png">
<meta property="article:published_time" content="2017-11-21T10:18:08.000Z">
<meta property="article:modified_time" content="2021-06-21T09:17:00.000Z">
<meta property="article:author" content="huagang">
<meta property="article:tag" content="kerberos">
<meta property="article:tag" content="hadoop">
<meta property="article:tag" content="hbase">
<meta property="article:tag" content="hive">
<meta property="article:tag" content="spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://qiniu.iclouds.work/FgvE_RlTJ6YsxhMw7m9lbDSKLPvd.png">


<link rel="canonical" href="http://blog.iclouds.work/2017/11/21/programming/%E8%BF%9E%E6%8E%A5%E5%B8%A6%E6%9C%89%20Kerberos%20%E8%AE%A4%E8%AF%81%E7%9A%84%20Hadoop%20%20HBase%20Spark/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://blog.iclouds.work/2017/11/21/programming/%E8%BF%9E%E6%8E%A5%E5%B8%A6%E6%9C%89%20Kerberos%20%E8%AE%A4%E8%AF%81%E7%9A%84%20Hadoop%20%20HBase%20Spark/","path":"2017/11/21/programming/连接带有 Kerberos 认证的 Hadoop  HBase Spark/","title":"连接带有 Kerberos 认证的 Hadoop Hbase Hive Spark"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>连接带有 Kerberos 认证的 Hadoop Hbase Hive Spark | Myblog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-200000008-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-200000008-1","only_pageview":false}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?c6fe2c57d92758c7a980d22eca71fd6a"></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Myblog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Hello World !</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%93%8D%E4%BD%9C%E7%8E%AF%E5%A2%83"><span class="nav-number">1.</span> <span class="nav-text">操作环境</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%87%86%E5%A4%87%E6%96%87%E4%BB%B6"><span class="nav-number">2.</span> <span class="nav-text">准备文件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Java-%E8%BF%9E%E6%8E%A5-Hadoop"><span class="nav-number">3.</span> <span class="nav-text">Java 连接 Hadoop</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Java-%E8%BF%9E%E6%8E%A5-HBase"><span class="nav-number">4.</span> <span class="nav-text">Java 连接 HBase</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Java-%E8%BF%9E%E6%8E%A5-Hive"><span class="nav-number">5.</span> <span class="nav-text">Java 连接 Hive</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Java-%E8%BF%9E%E6%8E%A5-Spark"><span class="nav-number">6.</span> <span class="nav-text">Java 连接 Spark</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Scala-%E8%BF%9E%E6%8E%A5-Spark"><span class="nav-number">7.</span> <span class="nav-text">Scala 连接 Spark</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%94%99%E8%AF%AF%E6%80%BB%E7%BB%93"><span class="nav-number">8.</span> <span class="nav-text">错误总结</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Can%E2%80%99t-get-Kerberos-realm"><span class="nav-number">8.1.</span> <span class="nav-text">1.  Can’t get Kerberos realm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-org-apache-hadoop-hbase-exceptions-ConnectionClosingException"><span class="nav-number">8.2.</span> <span class="nav-text">2. org.apache.hadoop.hbase.exceptions.ConnectionClosingException</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E8%BF%9E%E6%8E%A5-HBase-%E6%B2%A1%E6%9C%89%E5%8F%8D%E5%BA%94%EF%BC%8C%E9%87%8D%E5%A4%8D%E5%87%BA%E7%8E%B0-Call-exception"><span class="nav-number">8.3.</span> <span class="nav-text">3. 连接 HBase 没有反应，重复出现 Call exception</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-java-io-IOException-java-lang-reflect-InvocationTargetException"><span class="nav-number">8.4.</span> <span class="nav-text">4. java.io.IOException: java.lang.reflect.InvocationTargetException</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-SIMPLE-authentication-is-not-enabled-Available-TOKEN-KERBEROS"><span class="nav-number">8.5.</span> <span class="nav-text">5. SIMPLE authentication is not enabled.  Available:[TOKEN, KERBEROS]</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-Can%E2%80%99t-get-Master-Kerberos-principal-for-use-as-renewer"><span class="nav-number">8.6.</span> <span class="nav-text">6.  Can’t get Master Kerberos principal for use as renewer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-Secure-IO-is-not-possible-without-native-code-extensions"><span class="nav-number">8.7.</span> <span class="nav-text">7. Secure IO is not possible without native code extensions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-Exceeded-MAX-FAILED-UNIQUE-FETCHES-bailing-out"><span class="nav-number">8.8.</span> <span class="nav-text">8. Exceeded MAX_FAILED_UNIQUE_FETCHES; bailing-out</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-%E7%BB%88%E6%9E%81%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98"><span class="nav-number">8.9.</span> <span class="nav-text">9. 终极解决问题</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">huagang</p>
  <div class="site-description" itemprop="description">I Love You !</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">23</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/whg517" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;whg517" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/huagang517@126.com" title="E-Mail → huagang517@126.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/whg517" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://blog.iclouds.work/2017/11/21/programming/%E8%BF%9E%E6%8E%A5%E5%B8%A6%E6%9C%89%20Kerberos%20%E8%AE%A4%E8%AF%81%E7%9A%84%20Hadoop%20%20HBase%20Spark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="huagang">
      <meta itemprop="description" content="I Love You !">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Myblog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          连接带有 Kerberos 认证的 Hadoop Hbase Hive Spark
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-11-21 18:18:08" itemprop="dateCreated datePublished" datetime="2017-11-21T18:18:08+08:00">2017-11-21</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-06-21 17:17:00" itemprop="dateModified" datetime="2021-06-21T17:17:00+08:00">2021-06-21</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Hadoop/" itemprop="url" rel="index"><span itemprop="name">Hadoop</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>本文操作环境为使用 Windows 远程连接集群环境，通过代码带入 Kerberos 认证信息操作集群。</p>
<p>项目为 Maven Project，添加依赖仅限于测试连接，如做额外开发可能会缺少依赖。请自行添加</p>
<span id="more"></span>

<h2 id="操作环境"><a href="#操作环境" class="headerlink" title="操作环境"></a>操作环境</h2><ul>
<li>CentOS：7</li>
<li>Ambari：2.5.1.0</li>
<li>HDP：2.6.2.0-205</li>
<li>客户机：Windows 10</li>
</ul>
<h2 id="准备文件"><a href="#准备文件" class="headerlink" title="准备文件"></a>准备文件</h2><ul>
<li>Kerberos 配置文件。该文件一般位于 Kerberos 服务器的 <code>/etc/krb5.conf</code> 位置。将其下载到本地。</li>
<li>Hadoop 的配置文件。</li>
</ul>
<h2 id="Java-连接-Hadoop"><a href="#Java-连接-Hadoop" class="headerlink" title="Java 连接 Hadoop"></a>Java 连接 Hadoop</h2><p><a target="_blank" rel="noopener" href="https://www.2cto.com/kf/201204/126021.html">Hadoop Authentication</a></p>
<p>上面这篇博客讲解了 java 连接带有 Kerberos 的 HDFS 流程。可以看一下。</p>
<p>新建一个 Maven 项目</p>
<p><img src="http://qiniu.iclouds.work/FgvE_RlTJ6YsxhMw7m9lbDSKLPvd.png" alt="maven_project"></p>
<p>添加 HDFS 依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- HDFS dependences --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>将 <code>krb5.conf</code> <code>tendata.keytab</code>  放到项目根目录</p>
<p><img src="http://qiniu.iclouds.work/Fvr3toNl75tVQV6siXCnKq1bdQ_g.png" alt="keytab"></p>
<p>实例代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.LocatedFileStatus;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.RemoteIterator;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.security.UserGroupInformation;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by Administrator on 2017/11/21</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HadoopAuth</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span>&#123;</span><br><span class="line">        System.setProperty(<span class="string">&quot;java.security.krb5.conf&quot;</span>, <span class="string">&quot;krb5.conf&quot;</span>);</span><br><span class="line">        conf.set(<span class="string">&quot;HadoopAuth.security.authentication&quot;</span>, <span class="string">&quot;kerberos&quot;</span>);</span><br><span class="line">        conf.set(<span class="string">&quot;fs.defaultFS&quot;</span>, <span class="string">&quot;hdfs://192.168.10.1&quot;</span>);</span><br><span class="line">        UserGroupInformation.setConfiguration(conf);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            UserGroupInformation.loginUserFromKeytab(<span class="string">&quot;tendata@TENDATA.CN&quot;</span>, <span class="string">&quot;tendata.keytab&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        FileSystem fs = FileSystem.get(conf);</span><br><span class="line">        <span class="comment">// 传入当前认证用户有操作权限的路径</span></span><br><span class="line">        Path path = <span class="keyword">new</span> Path(<span class="string">&quot;/user/tendata/testdir&quot;</span>);</span><br><span class="line">        <span class="keyword">if</span> (! fs.exists(path))&#123;</span><br><span class="line">            <span class="keyword">boolean</span> mkdirs = fs.mkdirs(path);</span><br><span class="line">            System.out.println(<span class="string">&quot;mkdir &quot;</span> + mkdirs );</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">boolean</span> delete = fs.delete(path, <span class="keyword">true</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;delete &quot;</span> + delete);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Java-连接-HBase"><a href="#Java-连接-HBase" class="headerlink" title="Java 连接 HBase"></a>Java 连接 HBase</h2><p>添加 HBase 依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- HBase dependence --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-client --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.1.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.phoenix/phoenix-core --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.7.0-HBase-1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>实例代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.ClusterStatus;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.HBaseConfiguration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.ServerName;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Admin;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Connection;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.ConnectionFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.security.UserGroupInformation;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by Administrator on 2017/11/21</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseAuth</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> String zNode = <span class="string">&quot;m1.node.hadoop,m2.node.hadoop,m3.node.hadoop&quot;</span>;</span><br><span class="line">    <span class="keyword">static</span> String zPort = <span class="string">&quot;2181&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        Configuration conf = HBaseConfiguration.create();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// krb5.conf 文件位置随意。当不在项目中的时候需要引用绝对路径</span></span><br><span class="line">        System.setProperty(<span class="string">&quot;java.security.krb5.conf&quot;</span>, <span class="string">&quot;krb5.conf&quot;</span>);     <span class="comment">// 配置系统使用 krb5.conf</span></span><br><span class="line"></span><br><span class="line">        conf.set(<span class="string">&quot;hadoop.security.authentication&quot;</span>, <span class="string">&quot;Kerberos&quot;</span>);     <span class="comment">// 指定 HDFS 安全认证方式为 Kerberos。</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 下面的对 hdfs-site.xml  的配置都可以在集群配置文件中找得到的</span></span><br><span class="line">        conf.set(<span class="string">&quot;hbase.security.authentication&quot;</span>, <span class="string">&quot;kerberos&quot;</span>);      <span class="comment">// 指定 HBase 安全认证方式为 Kerberos。</span></span><br><span class="line">        conf.set(<span class="string">&quot;hbase.master.kerberos.principal&quot;</span>, <span class="string">&quot;hbase/_HOST@TENDATA.CN&quot;</span>);</span><br><span class="line">        conf.set(<span class="string">&quot;hbase.zookeeper.property.clientPort&quot;</span>, zPort);     <span class="comment">// Zookeeper 端口</span></span><br><span class="line">        conf.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, zNode);      <span class="comment">// Zookeeper 节点</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        在查看 Zookeeper 根目录信息会有两个 hbase 开头的目录</span></span><br><span class="line"><span class="comment">        /hbase-secure   在配置安全集群下，HBase 会使用这个</span></span><br><span class="line"><span class="comment">        /hbase-unsecure 在没有配置安全集群下，使用此目录</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        conf.set(<span class="string">&quot;zookeeper.znode.parent&quot;</span>, <span class="string">&quot;/hbase-secure&quot;</span>);    <span class="comment">// 指定 HBase 在 Zookeeper 目录</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 通过hadoop security下中的 UserGroupInformation类来实现使用keytab文件登录</span></span><br><span class="line"></span><br><span class="line">        UserGroupInformation.setConfiguration(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置登录的kerberos principal和对应的 keytab 文件，其中 keytab 文件需要kdc管理员生成给到开发人员</span></span><br><span class="line">        <span class="comment">// 文件如果没法放在项目目录中，则需要引用路径</span></span><br><span class="line">        UserGroupInformation.loginUserFromKeytab(<span class="string">&quot;tendata@TENDATA.CN&quot;</span>, <span class="string">&quot;tendata.keytab&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 与HBase数据库的连接对象</span></span><br><span class="line">        Connection connection = ConnectionFactory.createConnection(conf);</span><br><span class="line">        <span class="comment">// 数据库元数据操作对象</span></span><br><span class="line">        Admin admin = connection.getAdmin();</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;---------------获取集群信息-----------------&quot;</span>);</span><br><span class="line">        ClusterStatus clusterStatus = admin.getClusterStatus();</span><br><span class="line">        String id = clusterStatus.getClusterId();</span><br><span class="line">        String hbaseVersion = clusterStatus.getHBaseVersion();</span><br><span class="line">        ServerName master = clusterStatus.getMaster();</span><br><span class="line">        System.out.println(<span class="string">&quot;id:\t&quot;</span> + id +<span class="string">&quot;\n&quot;</span> +</span><br><span class="line">                            <span class="string">&quot;version:\t&quot;</span> + hbaseVersion + <span class="string">&quot;\n&quot;</span> +</span><br><span class="line">                            <span class="string">&quot;master:\t&quot;</span> + master</span><br><span class="line">        );</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Java-连接-Hive"><a href="#Java-连接-Hive" class="headerlink" title="Java 连接 Hive"></a>Java 连接 Hive</h2><p>添加 Hive 依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Hive dependence --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-jdbc --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-common --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.httpcomponents<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>httpclient<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>实例代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.security.UserGroupInformation;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.DriverManager;</span><br><span class="line"><span class="keyword">import</span> java.sql.ResultSet;</span><br><span class="line"><span class="keyword">import</span> java.sql.Statement;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by Administrator on 2017/11/21</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HiveAuth</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String driverName = <span class="string">&quot;org.apache.hive.jdbc.HiveDriver&quot;</span>;   <span class="comment">// Hive JDBC 驱动</span></span><br><span class="line">    <span class="comment">// url ，注意后面带有 principal</span></span><br><span class="line">    <span class="comment">// 这里使用 hive2 连接</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String url = <span class="string">&quot;jdbc:hive2://m1.node.hadoop:10000/default;principal=hive/m1.node.hadoop@TENDATA.CN;&quot;</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String user = <span class="string">&quot;hive&quot;</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String password = <span class="string">&quot;&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// krb5.conf 文件位置随意。当不在项目中的时候需要引用绝对路径</span></span><br><span class="line">        System.setProperty(<span class="string">&quot;java.security.krb5.conf&quot;</span>, <span class="string">&quot;krb5.conf&quot;</span>);     <span class="comment">// 配置系统使用 krb5.conf</span></span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.set(<span class="string">&quot;hadoop.security.authentication&quot;</span>, <span class="string">&quot;kerberos&quot;</span>); <span class="comment">// 指定 hadoop 安全认证方式为 Kerberos。</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 通过hadoop security下中的 UserGroupInformation类来实现使用keytab文件登录</span></span><br><span class="line">        UserGroupInformation.setConfiguration(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置登录的 kerberos principal和对应的 keytab 文件，其中 keytab 文件需要kdc管理员生成给到开发人员</span></span><br><span class="line">        <span class="comment">// 文件如果没法放在项目目录中，则需要引用路径</span></span><br><span class="line">        UserGroupInformation.loginUserFromKeytab(<span class="string">&quot;tendata@TENDATA.CN&quot;</span>, <span class="string">&quot;tendata.keytab&quot;</span>);</span><br><span class="line"></span><br><span class="line">        Class.forName(driverName);</span><br><span class="line">        Connection conn = DriverManager.getConnection(url, user, password);</span><br><span class="line">        Statement stmt = conn.createStatement();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ResultSet res = stmt.executeQuery(<span class="string">&quot;show databases&quot;</span>);</span><br><span class="line">        <span class="keyword">while</span> (res.next()) &#123;</span><br><span class="line">            System.out.println(res.getString(<span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Java-连接-Spark"><a href="#Java-连接-Spark" class="headerlink" title="Java 连接 Spark"></a>Java 连接 Spark</h2><p>使用 Java 连接 Spark 应该是少引用了某个参数，导致指定的安全认证参数传不进去。一直认定是 SIMPLE。</p>
<p>索性直接引入 hadoop 的配置文件。</p>
<p>在 Resource 目录下放入 <code>core-site.xml</code> <code>yarn-site.xml</code> 两个文件。</p>
<p><img src="http://qiniu.iclouds.work/FqF9JVuTAUIj3RWpkc0lP59QtHIa.png" alt="resource"></p>
<p>添加 Spark 依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Spark dependence --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.6.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>


<p>实例代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.security.UserGroupInformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by Administrator on 2017/11/21</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkAuth</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span>&#123;</span><br><span class="line">        <span class="comment">// 这里要给系统加载 krb5 的配置，否则系统会使用默认 remal</span></span><br><span class="line">        System.setProperty(<span class="string">&quot;java.security.krb5.conf&quot;</span>, <span class="string">&quot;krb5.conf&quot;</span>);</span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">// 下面的对 hdfs-site.xml  的配置都可以在集群配置文件中找得到的</span></span><br><span class="line">        configuration.set(<span class="string">&quot;hadoop.security.authentication&quot;</span>, <span class="string">&quot;kerberos&quot;</span>);     <span class="comment">// 指定 hadoop 安全认证方式为 Kerberos。</span></span><br><span class="line">        configuration.set(<span class="string">&quot;dfs.namenode.kerberos.principal.pattern&quot;</span>, <span class="string">&quot;nn/*@TENDATA.CN&quot;</span>);</span><br><span class="line">        configuration.set(<span class="string">&quot;fs.defaultFS&quot;</span>, <span class="string">&quot;hdfs://192.168.10.1&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 通过hadoop security下中的 UserGroupInformation类来实现使用keytab文件登录</span></span><br><span class="line">        UserGroupInformation.setConfiguration(configuration);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 设置登录的kerberos principal和对应的 keytab 文件，其中 keytab 文件需要kdc管理员生成给到开发人员</span></span><br><span class="line">            <span class="comment">// 文件如果没法放在项目目录中，则需要引用路径</span></span><br><span class="line">            UserGroupInformation.loginUserFromKeytab(<span class="string">&quot;tendata@TENDATA.CN&quot;</span>, <span class="string">&quot;tendata.keytab&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf().setAppName(<span class="string">&quot;Test_Java_E_Kevin&quot;</span>).setMaster(<span class="string">&quot;local&quot;</span>);</span><br><span class="line"></span><br><span class="line">        JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">        JavaRDD&lt;String&gt; stringJavaRDD = sc.textFile(<span class="string">&quot;hdfs://m1.node.hadoop:8020/user/tendata/tmp/tmp-kevin/example/data/example_data_id_name.txt&quot;</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        List&lt;String&gt; collect = stringJavaRDD.collect();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (String i : collect) &#123;</span><br><span class="line">            System.out.println(i);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Scala-连接-Spark"><a href="#Scala-连接-Spark" class="headerlink" title="Scala 连接 Spark"></a>Scala 连接 Spark</h2><p>针对于 Scala Spark 的环境请自行配置，具体连接代码如下。</p>
<p>需要在 Resource 目录下引用前面提到的两个配置文件</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.security.<span class="type">UserGroupInformation</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SQLContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkContext</span>, <span class="type">SparkConf</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Put</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="type">System</span>.setProperty(<span class="string">&quot;java.security.krb5.conf&quot;</span>, <span class="string">&quot;krb5.conf&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">Configuration</span>()</span><br><span class="line">    conf.set(<span class="string">&quot;hadoop.security.authentication&quot;</span>, <span class="string">&quot;Kerberos&quot;</span>)</span><br><span class="line">    conf.set(<span class="string">&quot;fs.defaultFS&quot;</span>, <span class="string">&quot;hdfs://192.168.10.1&quot;</span>)</span><br><span class="line">    conf.set(<span class="string">&quot;dfs.namenode.kerberos.principal.pattern&quot;</span>, <span class="string">&quot;nn/*@TENDATA.CN&quot;</span>)</span><br><span class="line">    <span class="type">UserGroupInformation</span>.setConfiguration(conf)</span><br><span class="line">    <span class="type">UserGroupInformation</span>.loginUserFromKeytab(<span class="string">&quot;tendata@TENDATA.CN&quot;</span>, <span class="string">&quot;tendata.keytab&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;Test_SparkReadHDFS_Kevin&quot;</span>).setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line">    <span class="keyword">val</span> scRDD = sc.textFile(<span class="string">&quot;hdfs://m1.node.hadoop:8020/user/tendata/tmp/tmp-kevin/example/data/example_data_id_name.txt&quot;</span>, <span class="number">1</span>);</span><br><span class="line">    scRDD.collect().foreach(x =&gt; println(x))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="错误总结"><a href="#错误总结" class="headerlink" title="错误总结"></a>错误总结</h2><h3 id="1-Can’t-get-Kerberos-realm"><a href="#1-Can’t-get-Kerberos-realm" class="headerlink" title="1.  Can’t get Kerberos realm"></a>1.  Can’t get Kerberos realm</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread <span class="string">&quot;main&quot;</span> java.lang.IllegalArgumentException: Can<span class="string">&#x27;t get Kerberos realm</span></span><br><span class="line"><span class="string">    at org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:65)</span></span><br><span class="line"><span class="string">    at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:249)</span></span><br><span class="line"><span class="string">    at org.apache.hadoop.security.UserGroupInformation.setConfiguration(UserGroupInformation.java:285)</span></span><br><span class="line"><span class="string">    at HdfsConnKerberos.HDFSClient.main(HDFSClient.java:43)</span></span><br><span class="line"><span class="string">    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span></span><br><span class="line"><span class="string">    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span></span><br><span class="line"><span class="string">    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span></span><br><span class="line"><span class="string">    at java.lang.reflect.Method.invoke(Method.java:497)</span></span><br><span class="line"><span class="string">    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)</span></span><br><span class="line"><span class="string">Caused by: java.lang.reflect.InvocationTargetException</span></span><br><span class="line"><span class="string">    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span></span><br><span class="line"><span class="string">    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span></span><br><span class="line"><span class="string">    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span></span><br><span class="line"><span class="string">    at java.lang.reflect.Method.invoke(Method.java:497)</span></span><br><span class="line"><span class="string">    at org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:84)</span></span><br><span class="line"><span class="string">    at org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:63)</span></span><br><span class="line"><span class="string">    ... 8 more</span></span><br><span class="line"><span class="string">Caused by: KrbException: Cannot locate default realm</span></span><br><span class="line"><span class="string">    at sun.security.krb5.Config.getDefaultRealm(Config.java:1006)</span></span><br><span class="line"><span class="string">    ... 14 more</span></span><br><span class="line"><span class="string">Caused by: KrbException: Generic error (description in e-text) (60) - Unable to locate Kerberos realm</span></span><br><span class="line"><span class="string">    at sun.security.krb5.Config.getRealmFromDNS(Config.java:1102)</span></span><br><span class="line"><span class="string">    at sun.security.krb5.Config.getDefaultRealm(Config.java:987)</span></span><br><span class="line"><span class="string">    ... 14 more</span></span><br></pre></td></tr></table></figure>

<p>主要几点</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Caused by: KrbException: Cannot locate default realm</span><br><span class="line"></span><br><span class="line">Caused by: KrbException: Generic error (description in e-text) (60) - Unable to locate Kerberos realm</span><br></pre></td></tr></table></figure>

<p>主要原因在代码中没有添加 <code>krb5.conf</code> 这个配置，所以检查这个配置文件的是否存在和文件内容的正确性</p>
<h3 id="2-org-apache-hadoop-hbase-exceptions-ConnectionClosingException"><a href="#2-org-apache-hadoop-hbase-exceptions-ConnectionClosingException" class="headerlink" title="2. org.apache.hadoop.hbase.exceptions.ConnectionClosingException"></a>2. org.apache.hadoop.hbase.exceptions.ConnectionClosingException</h3><p>读取 HBase 信息，连接到 HBase 没反应，重复出现下面的信息。</p>
<!-- markdownlint-disable MD013 MD033-->
<p>Call exception, tries=10, retries=31, started=48283 ms ago, cancelled=false, msg=com.google.protobuf.ServiceException: org.apache.hadoop.hbase.exceptions.ConnectionClosingException: Call to m1.node.hadoop/192.168.10.1:16000 failed on local exception: org.apache.hadoop.hbase.exceptions.ConnectionClosingException: Connection to m1.node.hadoop/192.168.10.1:16000 is closing. Call id=10, waitTime=1</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">2017-10-13 15:26:02,021 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:&lt;init&gt;(438)) - Initiating client connection, connectString=m1.node.hadoop:2181,m2.node.hadoop:2181,m3.node.hadoop:2181 sessionTimeout=180000 watcher=org.apache.hadoop.hbase.zookeeper.PendingWatcher@65b3f4a4</span><br><span class="line">2017-10-13 15:26:02,077 INFO  [main-SendThread(m3.node.hadoop:2181)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1019)) - Opening socket connection to server m3.node.hadoop/192.168.10.3:2181. Will not attempt to authenticate using SASL (unknown error)</span><br><span class="line">2017-10-13 15:26:02,079 INFO  [main-SendThread(m3.node.hadoop:2181)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(864)) - Socket connection established, initiating session, client: /192.168.2.199:59415, server: m3.node.hadoop/192.168.10.3:2181</span><br><span class="line">2017-10-13 15:26:02,104 INFO  [main-SendThread(m3.node.hadoop:2181)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1279)) - Session establishment complete on server m3.node.hadoop/192.168.10.3:2181, sessionid = 0x35efece455700dd, negotiated timeout = 40000</span><br><span class="line">2017-10-13 15:26:05,349 WARN  [main]  (NetworkAddressUtils.java:getLocalIpAddress(389)) - Your hostname, DESKTOP-5KM5T43 resolves to a loopback/non-reachable address: fe80:0:0:0:3433:e0f1:9aa7:18da%net4, but we couldn&#x27;t find any external IP address!</span><br><span class="line">2017-10-13 15:26:06,524 WARN  [main] shortcircuit.DomainSocketFactory (DomainSocketFactory.java:&lt;init&gt;(117)) - The short-circuit local reads feature cannot be used because UNIX Domain sockets are not available on Windows.</span><br><span class="line">2017-10-13 15:26:54,855 INFO  [main] client.RpcRetryingCaller (RpcRetryingCaller.java:callWithRetries(146)) - Call exception, tries=10, retries=31, started=48283 ms ago, cancelled=false, msg=com.google.protobuf.ServiceException: org.apache.hadoop.hbase.exceptions.ConnectionClosingException: Call to m1.node.hadoop/192.168.10.1:16000 failed on local exception: org.apache.hadoop.hbase.exceptions.ConnectionClosingException: Connection to m1.node.hadoop/192.168.10.1:16000 is closing. Call id=10, waitTime=1 </span><br><span class="line">2017-10-13 15:27:15,012 INFO  [main] client.RpcRetryingCaller (RpcRetryingCaller.java:callWithRetries(146)) - Call exception, tries=11, retries=31, started=68440 ms ago, cancelled=false, msg=com.google.protobuf.ServiceException: org.apache.hadoop.hbase.exceptions.ConnectionClosingException: Call to m1.node.hadoop/192.168.10.1:16000 failed on local exception: org.apache.hadoop.hbase.exceptions.ConnectionClosingException: Connection to m1.node.hadoop/192.168.10.1:16000 is closing. Call id=11, waitTime=1 </span><br></pre></td></tr></table></figure>
<!-- markdownlint-restore -->

<p>这种情况是在创建 Configuration 的格式不对造成。</p>
<p>下面两种获取 Configuration 对象是不同的。</p>
<p>如果使用第一种 conf 操作 HBase，则会出现上述错误。</p>
<p>而正确的创建方式应该是第二种。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">Configuration conf = HBaseConfiguration.create();</span><br></pre></td></tr></table></figure>

<h3 id="3-连接-HBase-没有反应，重复出现-Call-exception"><a href="#3-连接-HBase-没有反应，重复出现-Call-exception" class="headerlink" title="3. 连接 HBase 没有反应，重复出现 Call exception"></a>3. 连接 HBase 没有反应，重复出现 Call exception</h3><!-- markdownlint-disable MD013 MD033-->
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">2017-11-21 17:45:11,982 INFO [org.apache.zookeeper.ZooKeeper] - Initiating client connection, connectString=m1.node.hadoop:2181,m2.node.hadoop:2181,m3.node.hadoop:2181 sessionTimeout=90000 watcher=hconnection-0x66d189790x0, quorum=m1.node.hadoop:2181,m2.node.hadoop:2181,m3.node.hadoop:2181, baseZNode=/hbase-secure</span><br><span class="line">2017-11-21 17:45:12,049 INFO [org.apache.zookeeper.ClientCnxn] - Opening socket connection to server m2.node.hadoop/192.168.10.2:2181. Will not attempt to authenticate using SASL (unknown error)</span><br><span class="line">2017-11-21 17:45:12,050 INFO [org.apache.zookeeper.ClientCnxn] - Socket connection established to m2.node.hadoop/192.168.10.2:2181, initiating session</span><br><span class="line">2017-11-21 17:45:12,065 INFO [org.apache.zookeeper.ClientCnxn] - Session establishment complete on server m2.node.hadoop/192.168.10.2:2181, sessionid = 0x25fdc95c59d001d, negotiated timeout = 40000</span><br><span class="line">2017-11-21 17:45:15,386 WARN [] - Your hostname, DESKTOP-5KM5T43 resolves to a loopback/non-reachable address: fe80:0:0:0:3c56:c61d:8b18:745%net4, but we couldn&#x27;t find any external IP address!</span><br><span class="line">---------------获取集群信息-----------------</span><br><span class="line">2017-11-21 17:45:54,986 INFO [org.apache.hadoop.hbase.client.RpcRetryingCaller] - Call exception, tries=10, retries=35, started=38401 ms ago, cancelled=false, msg=</span><br><span class="line">2017-11-21 17:46:04,998 INFO [org.apache.hadoop.hbase.client.RpcRetryingCaller] - Call exception, tries=11, retries=35, started=48413 ms ago, cancelled=false, msg=</span><br></pre></td></tr></table></figure>
<!-- markdownlint-restore -->
<p>这是没有指定 HBase 安全认证导致的。</p>
<p>增加如下配置即可</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conf.set(<span class="string">&quot;hbase.security.authentication&quot;</span>, <span class="string">&quot;kerberos&quot;</span>);      <span class="comment">// 指定 HBase 安全认证方式为 Kerberos。</span></span><br></pre></td></tr></table></figure>

<h3 id="4-java-io-IOException-java-lang-reflect-InvocationTargetException"><a href="#4-java-io-IOException-java-lang-reflect-InvocationTargetException" class="headerlink" title="4. java.io.IOException: java.lang.reflect.InvocationTargetException"></a>4. java.io.IOException: java.lang.reflect.InvocationTargetException</h3><p>连接 HBase 出现 <code>org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory</code> 类找不到</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread <span class="string">&quot;main&quot;</span> java.io.IOException: java.lang.reflect.InvocationTargetException</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:<span class="number">240</span>)</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:<span class="number">218</span>)</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:<span class="number">119</span>)</span><br><span class="line">    at HBaseAuth.main(HBaseAuth.java:<span class="number">33</span>)</span><br><span class="line">    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:<span class="number">62</span>)</span><br><span class="line">    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:<span class="number">43</span>)</span><br><span class="line">    at java.lang.reflect.Method.invoke(Method.java:<span class="number">497</span>)</span><br><span class="line">    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:<span class="number">144</span>)</span><br><span class="line">Caused by: java.lang.reflect.InvocationTargetException</span><br><span class="line">    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class="line">    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:<span class="number">62</span>)</span><br><span class="line">    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:<span class="number">45</span>)</span><br><span class="line">    at java.lang.reflect.Constructor.newInstance(Constructor.java:<span class="number">422</span>)</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:<span class="number">238</span>)</span><br><span class="line">    ... <span class="number">8</span> more</span><br><span class="line">Caused by: java.lang.UnsupportedOperationException: Unable to find org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory</span><br><span class="line">    at org.apache.hadoop.hbase.util.ReflectionUtils.instantiateWithCustomCtor(ReflectionUtils.java:<span class="number">36</span>)</span><br><span class="line">    at org.apache.hadoop.hbase.ipc.RpcControllerFactory.instantiate(RpcControllerFactory.java:<span class="number">58</span>)</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.createAsyncProcess(ConnectionManager.java:<span class="number">2256</span>)</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.&lt;init&gt;(ConnectionManager.java:<span class="number">691</span>)</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.&lt;init&gt;(ConnectionManager.java:<span class="number">631</span>)</span><br><span class="line">    ... <span class="number">13</span> more</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory</span><br><span class="line">    at java.net.URLClassLoader.findClass(URLClassLoader.java:<span class="number">381</span>)</span><br><span class="line">    at java.lang.ClassLoader.loadClass(ClassLoader.java:<span class="number">424</span>)</span><br><span class="line">    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:<span class="number">331</span>)</span><br><span class="line">    at java.lang.ClassLoader.loadClass(ClassLoader.java:<span class="number">357</span>)</span><br><span class="line">    at java.lang.Class.forName0(Native Method)</span><br><span class="line">    at java.lang.Class.forName(Class.java:<span class="number">264</span>)</span><br><span class="line">    at org.apache.hadoop.hbase.util.ReflectionUtils.instantiateWithCustomCtor(ReflectionUtils.java:<span class="number">32</span>)</span><br><span class="line">    ... <span class="number">17</span> more</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/itboys/p/6862366.html">https://www.cnblogs.com/itboys/p/6862366.html</a></p>
<ol>
<li><p>检查应用开发工程的配置文件hbase-site.xml中是否包含配置项hbase.rpc.controllerfactory.class。</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;name&gt;hbase.rpc.controllerfactory.class&lt;/name&gt;</span><br><span class="line">&lt;value&gt;org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory&lt;/value&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>如果当前的应用开发工程配置项中包含该配置项，则应用开发程序还需要引入Jar包“phoenix-core-4.4.0-HBase-1.0.jar”。此Jar包可以从HBase客户端安装目录下的“HBase/hbase/lib”获取。</p>
</li>
<li><p>如果不想引入该Jar包，请将应用开发工程的配置文件“hbase-site.xml”中的配置“hbase.rpc.controllerfactory.class”删除掉。</p>
</li>
</ol>
<h3 id="5-SIMPLE-authentication-is-not-enabled-Available-TOKEN-KERBEROS"><a href="#5-SIMPLE-authentication-is-not-enabled-Available-TOKEN-KERBEROS" class="headerlink" title="5. SIMPLE authentication is not enabled.  Available:[TOKEN, KERBEROS]"></a>5. SIMPLE authentication is not enabled.  Available:[TOKEN, KERBEROS]</h3><p>这个问题我只在使用 Spark 的情况下出现过。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread <span class="string">&quot;main&quot;</span> org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled.  Available:[TOKEN, KERBEROS]</span><br><span class="line">    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class="line">    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:<span class="number">62</span>)</span><br><span class="line">    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:<span class="number">45</span>)</span><br><span class="line">    at java.lang.reflect.Constructor.newInstance(Constructor.java:<span class="number">422</span>)</span><br><span class="line">    at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:<span class="number">106</span>)</span><br><span class="line">    at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:<span class="number">73</span>)</span><br><span class="line">    at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:<span class="number">2110</span>)</span><br><span class="line">    at org.apache.hadoop.hdfs.DistributedFileSystem$<span class="number">22.</span>doCall(DistributedFileSystem.java:<span class="number">1305</span>)</span><br><span class="line">    at org.apache.hadoop.hdfs.DistributedFileSystem$<span class="number">22.</span>doCall(DistributedFileSystem.java:<span class="number">1301</span>)</span><br><span class="line">    at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:<span class="number">81</span>)</span><br><span class="line">    at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:<span class="number">1317</span>)</span><br><span class="line">    at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:<span class="number">57</span>)</span><br><span class="line">    at org.apache.hadoop.fs.Globber.glob(Globber.java:<span class="number">252</span>)</span><br><span class="line">    at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:<span class="number">1674</span>)</span><br><span class="line">    at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:<span class="number">259</span>)</span><br><span class="line">    at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:<span class="number">229</span>)</span><br><span class="line">    at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:<span class="number">315</span>)</span><br><span class="line">    at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:<span class="number">200</span>)</span><br><span class="line">    at org.apache.spark.rdd.RDD$$anonfun$partitions$<span class="number">2.</span>apply(RDD.scala:<span class="number">248</span>)</span><br><span class="line">    at org.apache.spark.rdd.RDD$$anonfun$partitions$<span class="number">2.</span>apply(RDD.scala:<span class="number">246</span>)</span><br><span class="line">    at scala.Option.getOrElse(Option.scala:<span class="number">121</span>)</span><br><span class="line">    at org.apache.spark.rdd.RDD.partitions(RDD.scala:<span class="number">246</span>)</span><br><span class="line">    at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:<span class="number">35</span>)</span><br><span class="line">    at org.apache.spark.rdd.RDD$$anonfun$partitions$<span class="number">2.</span>apply(RDD.scala:<span class="number">248</span>)</span><br><span class="line">    at org.apache.spark.rdd.RDD$$anonfun$partitions$<span class="number">2.</span>apply(RDD.scala:<span class="number">246</span>)</span><br><span class="line">    at scala.Option.getOrElse(Option.scala:<span class="number">121</span>)</span><br><span class="line">    at org.apache.spark.rdd.RDD.partitions(RDD.scala:<span class="number">246</span>)</span><br><span class="line">    at org.apache.spark.SparkContext.runJob(SparkContext.scala:<span class="number">1911</span>)</span><br><span class="line">    at org.apache.spark.rdd.RDD$$anonfun$foreach$<span class="number">1.</span>apply(RDD.scala:<span class="number">875</span>)</span><br><span class="line">    at org.apache.spark.rdd.RDD$$anonfun$foreach$<span class="number">1.</span>apply(RDD.scala:<span class="number">873</span>)</span><br><span class="line">    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:<span class="number">151</span>)</span><br><span class="line">    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:<span class="number">112</span>)</span><br><span class="line">    at org.apache.spark.rdd.RDD.withScope(RDD.scala:<span class="number">358</span>)</span><br><span class="line">    at org.apache.spark.rdd.RDD.foreach(RDD.scala:<span class="number">873</span>)</span><br><span class="line">    at HDFS.SparkHDFS$.main(SparkHDFS.scala:<span class="number">43</span>)</span><br><span class="line">    at HDFS.SparkHDFS.main(SparkHDFS.scala)</span><br><span class="line">    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:<span class="number">62</span>)</span><br><span class="line">    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:<span class="number">43</span>)</span><br><span class="line">    at java.lang.reflect.Method.invoke(Method.java:<span class="number">497</span>)</span><br><span class="line">    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:<span class="number">144</span>)</span><br><span class="line">Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): SIMPLE authentication is not enabled.  Available:[TOKEN, KERBEROS]</span><br><span class="line">    at org.apache.hadoop.ipc.Client.call(Client.java:<span class="number">1475</span>)</span><br><span class="line">    at org.apache.hadoop.ipc.Client.call(Client.java:<span class="number">1412</span>)</span><br><span class="line">    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:<span class="number">229</span>)</span><br><span class="line">    at com.sun.proxy.$Proxy20.getFileInfo(Unknown Source)</span><br><span class="line">    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:<span class="number">771</span>)</span><br><span class="line">    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:<span class="number">62</span>)</span><br><span class="line">    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:<span class="number">43</span>)</span><br><span class="line">    at java.lang.reflect.Method.invoke(Method.java:<span class="number">497</span>)</span><br><span class="line">    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:<span class="number">191</span>)</span><br><span class="line">    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:<span class="number">102</span>)</span><br><span class="line">    at com.sun.proxy.$Proxy21.getFileInfo(Unknown Source)</span><br><span class="line">    at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:<span class="number">2108</span>)</span><br><span class="line">    ... <span class="number">34</span> more</span><br></pre></td></tr></table></figure>

<p>直接在 Resource 目录下加入 <code>core-site.xml</code> 配置文件即可。</p>
<p>当人加入了上述配置文件后，重新运行又会出现下面的错误</p>
<h3 id="6-Can’t-get-Master-Kerberos-principal-for-use-as-renewer"><a href="#6-Can’t-get-Master-Kerberos-principal-for-use-as-renewer" class="headerlink" title="6.  Can’t get Master Kerberos principal for use as renewer"></a>6.  Can’t get Master Kerberos principal for use as renewer</h3><p>记不得这个错误有没有在其他地方出现过了。反正现在在使用 Spark 的时候出现了。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread <span class="string">&quot;main&quot;</span> java.io.IOException: Can<span class="string">&#x27;t get Master Kerberos principal for use as renewer</span></span><br><span class="line"><span class="string">    at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:116)</span></span><br><span class="line"><span class="string">    at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:100)</span></span><br><span class="line"><span class="string">    at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80)</span></span><br><span class="line"><span class="string">    at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:205)</span></span><br><span class="line"><span class="string">    at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)</span></span><br><span class="line"><span class="string">    at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:202)</span></span><br><span class="line"><span class="string">    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)</span></span><br><span class="line"><span class="string">    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)</span></span><br><span class="line"><span class="string">    at scala.Option.getOrElse(Option.scala:121)</span></span><br><span class="line"><span class="string">    at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)</span></span><br><span class="line"><span class="string">    at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)</span></span><br><span class="line"><span class="string">    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)</span></span><br><span class="line"><span class="string">    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)</span></span><br><span class="line"><span class="string">    at scala.Option.getOrElse(Option.scala:121)</span></span><br><span class="line"><span class="string">    at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)</span></span><br><span class="line"><span class="string">    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)</span></span><br><span class="line"><span class="string">    at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)</span></span><br><span class="line"><span class="string">    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)</span></span><br><span class="line"><span class="string">    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)</span></span><br><span class="line"><span class="string">    at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)</span></span><br><span class="line"><span class="string">    at org.apache.spark.rdd.RDD.collect(RDD.scala:926)</span></span><br><span class="line"><span class="string">    at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:339)</span></span><br><span class="line"><span class="string">    at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:46)</span></span><br><span class="line"><span class="string">    at SparkAuth.main(SparkAuth.java:43)</span></span><br><span class="line"><span class="string">    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span></span><br><span class="line"><span class="string">    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span></span><br><span class="line"><span class="string">    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span></span><br><span class="line"><span class="string">    at java.lang.reflect.Method.invoke(Method.java:497)</span></span><br><span class="line"><span class="string">    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)</span></span><br></pre></td></tr></table></figure>

<p>解决办法：</p>
<p>在 Resource 目录下引入 <code>yarn-site.xml</code> 配置文件</p>
<p><strong>针对前两个错误，主要在 Spark 中出现。本人猜测，可能是因为在 Windows 端运行 local 模式程序的时候，本地作为 Driver，当 Executor 端 真正去访问 HDFS 中的资源的时候， Executor 并没有拿到认证身份。所以，在加入配置文件后， Executor 端会通过配置去相应位置使用 keytab 获取 kgt ，然后正常访问集群中的资源。</strong></p>
<blockquote>
<p>Update 2020-04-17 16:22:00</p>
</blockquote>
<p>在开发端远程提交 mapreduce 任务的时候同样出现了这个问题。在此确认是由于 yarn 的认证问题。</p>
<p>解决方案：</p>
<p>增加 <code>yarn</code> 相关 principal。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">configuration.set(<span class="string">&quot;yarn.nodemanager.principal&quot;</span>, <span class="string">&quot;nm/_HOST@TENDATA.CN&quot;</span>);</span><br><span class="line">configuration.set(<span class="string">&quot;yarn.resourcemanager.principal&quot;</span>, <span class="string">&quot;rm/_HOST@TENDATA.CN&quot;</span>);</span><br></pre></td></tr></table></figure>

<h3 id="7-Secure-IO-is-not-possible-without-native-code-extensions"><a href="#7-Secure-IO-is-not-possible-without-native-code-extensions" class="headerlink" title="7. Secure IO is not possible without native code extensions"></a>7. Secure IO is not possible without native code extensions</h3><p>运行 mapreduce 任务，出现如下问题</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">java.lang.Exception: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in localfetcher#1</span><br><span class="line">    at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:<span class="number">492</span>)</span><br><span class="line">    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:<span class="number">559</span>)</span><br><span class="line">Caused by: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in localfetcher#1</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:<span class="number">134</span>)</span><br><span class="line">    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:<span class="number">377</span>)</span><br><span class="line">    at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:<span class="number">347</span>)</span><br><span class="line">    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:<span class="number">511</span>)</span><br><span class="line">    at java.util.concurrent.FutureTask.run(FutureTask.java:<span class="number">266</span>)</span><br><span class="line">    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:<span class="number">1149</span>)</span><br><span class="line">    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:<span class="number">624</span>)</span><br><span class="line">    at java.lang.Thread.run(Thread.java:<span class="number">748</span>)</span><br><span class="line">Caused by: java.lang.ExceptionInInitializerError</span><br><span class="line">    at org.apache.hadoop.mapred.SpillRecord.&lt;init&gt;(SpillRecord.java:<span class="number">71</span>)</span><br><span class="line">    at org.apache.hadoop.mapred.SpillRecord.&lt;init&gt;(SpillRecord.java:<span class="number">62</span>)</span><br><span class="line">    at org.apache.hadoop.mapred.SpillRecord.&lt;init&gt;(SpillRecord.java:<span class="number">57</span>)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.LocalFetcher.copyMapOutput(LocalFetcher.java:<span class="number">125</span>)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.LocalFetcher.doCopy(LocalFetcher.java:<span class="number">103</span>)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.LocalFetcher.run(LocalFetcher.java:<span class="number">86</span>)</span><br><span class="line">Caused by: java.lang.RuntimeException: Secure IO is not possible without <span class="keyword">native</span> code extensions.</span><br><span class="line">    at org.apache.hadoop.io.SecureIOUtils.&lt;clinit&gt;(SecureIOUtils.java:<span class="number">71</span>)</span><br><span class="line">    ... <span class="number">6</span> more</span><br><span class="line"><span class="number">2020</span>-<span class="number">04</span>-<span class="number">17</span> <span class="number">20</span>:<span class="number">16</span></span><br></pre></td></tr></table></figure>

<p>这个问题是由于没有使用 <code>Tative library</code> 导致的，在程序运行第一行一般会有个警告 <code>Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</code><br>这是没有加载动态库。一般情况下 Hadoop 的二进制报已经包含了，只是没有在系统中加载。</p>
<p>动态库在 <code>$HADOOP_HOME/lib/native</code> 里面，没有的要去找一下，或者自己编译。</p>
<p>有了动态库还需要在系统中配置，推荐是配置环境变量：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native</span><br><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HADOOP_COMMON_LIB_NATIVE_DIR</span><br></pre></td></tr></table></figure>

<p>环境变量生效后就可以看到警告消除，而且程序也不会出现上面提到的问题啦。</p>
<h3 id="8-Exceeded-MAX-FAILED-UNIQUE-FETCHES-bailing-out"><a href="#8-Exceeded-MAX-FAILED-UNIQUE-FETCHES-bailing-out" class="headerlink" title="8. Exceeded MAX_FAILED_UNIQUE_FETCHES; bailing-out"></a>8. Exceeded MAX_FAILED_UNIQUE_FETCHES; bailing-out</h3><p>这个问题是在使用了 Kerberos 认证的时候出现的，如果没有使用认证也出现了请参考<br><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/24066128/hadoop-error-in-shuffle-in-fetcher-exceeded-max-failed-unique-fetches">Hadoop error in shuffle in fetcher: Exceeded MAX_FAILED_UNIQUE_FETCHES</a> 解决。</p>
<p>出现这个问题是在集群客户端使用 <code>yarn jar demo.jar</code> 在集群中运行 mapreduce 任务的时候报错了。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#30</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:<span class="number">134</span>)</span><br><span class="line">    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:<span class="number">377</span>)</span><br><span class="line">    at org.apache.hadoop.mapred.YarnChild$<span class="number">2.</span>run(YarnChild.java:<span class="number">174</span>)</span><br><span class="line">    at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">    at javax.security.auth.Subject.doAs(Subject.java:<span class="number">422</span>)</span><br><span class="line">    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:<span class="number">1730</span>)</span><br><span class="line">    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:<span class="number">168</span>)</span><br><span class="line">Caused by: java.io.IOException: Exceeded MAX_FAILED_UNIQUE_FETCHES; bailing-out.</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.checkReducerHealth(ShuffleSchedulerImpl.java:<span class="number">396</span>)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.copyFailed(ShuffleSchedulerImpl.java:<span class="number">311</span>)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:<span class="number">361</span>)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:<span class="number">198</span>)</span><br></pre></td></tr></table></figure>

<p>当时想到运行 mapreduce 任务需要增加相关认证信息，但是没有加完整。完整认证代码如下</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> demo.utils;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.security.UserGroupInformation;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Auth</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String keytab;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Auth</span><span class="params">(String keytab)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.keytab = keytab;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">authorization</span><span class="params">(Configuration configuration)</span> </span>&#123;</span><br><span class="line">        System.setProperty(<span class="string">&quot;java.security.krb5.conf&quot;</span>, <span class="string">&quot;/etc/krb5.conf&quot;</span>);</span><br><span class="line">        configuration.set(<span class="string">&quot;hadoop.security.authentication&quot;</span>, <span class="string">&quot;Kerberos&quot;</span>);</span><br><span class="line">        configuration.set(<span class="string">&quot;fs.defaultFS&quot;</span>, <span class="string">&quot;hdfs://m1.node.hadoop&quot;</span>);</span><br><span class="line">        configuration.set(<span class="string">&quot;dfs.namenode.kerberos.principal.pattern&quot;</span>, <span class="string">&quot;nn/*@TENDATA.CN&quot;</span>);</span><br><span class="line">        configuration.set(<span class="string">&quot;yarn.nodemanager.principal&quot;</span>, <span class="string">&quot;nm/_HOST@TENDATA.CN&quot;</span>);</span><br><span class="line">        configuration.set(<span class="string">&quot;yarn.resourcemanager.principal&quot;</span>, <span class="string">&quot;rm/_HOST@TENDATA.CN&quot;</span>);</span><br><span class="line">        configuration.set(<span class="string">&quot;yarn.nodemanager.webapp.spnego-principal&quot;</span>, <span class="string">&quot;HTTP/_HOST@TENDATA.CN&quot;</span>);     <span class="comment">// 集群运行的时候一定要加，在执行 shuffle 的时候会在集群间交换数据</span></span><br><span class="line">        configuration.set(<span class="string">&quot;yarn.resourcemanager.webapp.spnego-principal&quot;</span>, <span class="string">&quot;HTTP/_HOST@TENDATA.CN&quot;</span>); <span class="comment">// 这也也要加上</span></span><br><span class="line">        UserGroupInformation.setConfiguration(configuration);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            UserGroupInformation.setConfiguration(configuration);</span><br><span class="line">            UserGroupInformation.loginUserFromKeytab(<span class="string">&quot;whg@TENDATA.CN&quot;</span>, <span class="keyword">this</span>.keytab);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>代码如上，在运行 mapreduce 任务的时候除了增加 <code>yarn.nodemanager.principal</code> 和 <code>yarn.resourcemanager.principal</code> 的配置还要增加 <code>yarn.nodemanager.webapp.spnego-principal</code><br> 和 <code>yarn.resourcemanager.webapp.spnego-principal</code> 的配置，否则集群间交换数据会报上面的问题。在开发环境即 local 模式运行的时候可以不用后面两个。</p>
<h3 id="9-终极解决问题"><a href="#9-终极解决问题" class="headerlink" title="9. 终极解决问题"></a>9. 终极解决问题</h3><p>在 Resource 目录引入所使用的服务的所有配置。比如 hadoop 的四个配置文件。</p>
<p>然后使用代码直接加载配置</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">configuration.addResource(<span class="string">&quot;core-site.xml&quot;</span>);</span><br><span class="line">configuration.addResource(<span class="string">&quot;hdfs-site.xml&quot;</span>);</span><br><span class="line">configuration.addResource(<span class="string">&quot;mapred-site.xml&quot;</span>);</span><br><span class="line">configuration.addResource(<span class="string">&quot;yarn-site.xml&quot;</span>);</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/kerberos/" rel="tag"># kerberos</a>
              <a href="/tags/hadoop/" rel="tag"># hadoop</a>
              <a href="/tags/hbase/" rel="tag"># hbase</a>
              <a href="/tags/hive/" rel="tag"># hive</a>
              <a href="/tags/spark/" rel="tag"># spark</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2017/11/20/operations/Ambari%20%E6%95%B4%E5%90%88%20Kerberos%20+%20Openldap/" rel="prev" title="Ambari 整合 Kerberos + Openldap">
                  <i class="fa fa-chevron-left"></i> Ambari 整合 Kerberos + Openldap
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2018/03/23/operations/Deepin%20%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/" rel="next" title="Deepin 使用记录">
                  Deepin 使用记录 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2017 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">huagang</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
